{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl\n",
    "import cross_validation as cv\n",
    "import hyperparameter_opti as hpopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, train_labels, ids = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize data\n",
    "new_data = helpers.standardize(helpers.clean_data(train_data))\n",
    "train_labels[train_labels==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add bias to data:\n",
    "new_data = np.c_[np.ones((new_data.shape[0], 1)), new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data:\n",
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "#Standardize test data:\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))\n",
    "#Add bias to test data:\n",
    "tx_test = np.c_[np.ones((tx_test.shape[0], 1)), tx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least Squares:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ls, loss_ls = impl.least_squares(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = impl.test_data(weights_ls, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_LS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best degree to build a polynomial basis from the training data:\n",
    "degree_opt, lambda_opt = cv.best_degree_selection_ridge(train_labels, new_data, np.arange(2,5), 4, np.logspace(-4,0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(degree_opt)\n",
    "print(lambda_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(new_data.shape)\n",
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 63)\n",
      "(63,)\n"
     ]
    }
   ],
   "source": [
    "#build polynomial basis from the entire training set using the optimal degree:\n",
    "xpoly = cv.build_poly(new_data, degree_opt)\n",
    "print(xpoly.shape)\n",
    "#compute weights and loss for the optimal lambda:\n",
    "w_ridge, loss_ridge = impl.ridge_regression(train_labels, xpoly, lambda_opt)\n",
    "print(w_ridge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 63)\n"
     ]
    }
   ],
   "source": [
    "#build polynomial basis from the test set:\n",
    "xpoly_test = cv.build_poly(tx_test, degree_opt)\n",
    "print(xpoly_test.shape)\n",
    "#Compute predicted labels:\n",
    "y_test = impl.test_data(w_ridge, xpoly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_RG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice into training and validation sets\n",
    "y_validation, y_train, tx_validation, tx_train = helpers.slice_data(train_labels, new_data, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [tx_train.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current gamma=1e-06\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.747363250564084\n",
      "Current iteration=200, loss=0.7473366665161033\n",
      "Current iteration=300, loss=0.7473100868077831\n",
      "Current iteration=400, loss=0.7472835114384289\n",
      "Current iteration=500, loss=0.7472569404073467\n",
      "Current iteration=600, loss=0.7472303737138422\n",
      "Current iteration=700, loss=0.7472038113572215\n",
      "Current iteration=800, loss=0.7471772533367894\n",
      "Current iteration=900, loss=0.7471506996518528\n",
      "training_loss = 0.7471241503017174, validation_loss = 0.51774464810816\n",
      "Current gamma=7.498942093324558e-06\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.7471905593727567\n",
      "Current iteration=200, loss=0.746991523616922\n",
      "Current iteration=300, loss=0.7467927313922089\n",
      "Current iteration=400, loss=0.7465941824059665\n",
      "Current iteration=500, loss=0.7463958763655983\n",
      "Current iteration=600, loss=0.7461978129785657\n",
      "Current iteration=700, loss=0.745999991952388\n",
      "Current iteration=800, loss=0.7458024129946459\n",
      "Current iteration=900, loss=0.74560507581298\n",
      "training_loss = 0.7454079801150958, validation_loss = 0.5171604857204439\n",
      "Current gamma=5.623413251903491e-05\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.7459013518162192\n",
      "Current iteration=200, loss=0.7444264696094288\n",
      "Current iteration=300, loss=0.7429650691455109\n",
      "Current iteration=400, loss=0.7415170274691691\n",
      "Current iteration=500, loss=0.740082221890034\n",
      "Current iteration=600, loss=0.7386605300156371\n",
      "Current iteration=700, loss=0.7372518297836432\n",
      "Current iteration=800, loss=0.7358559994933068\n",
      "Current iteration=900, loss=0.7344729178361207\n",
      "training_loss = 0.733102463925637, validation_loss = 0.5130802368926275\n",
      "Current gamma=0.00042169650342858224\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.7365522431249277\n",
      "Current iteration=200, loss=0.7264353124448658\n",
      "Current iteration=300, loss=0.716988673789292\n",
      "Current iteration=400, loss=0.7081638427067406\n",
      "Current iteration=500, loss=0.6999146881202951\n",
      "Current iteration=600, loss=0.6921977397398339\n",
      "Current iteration=700, loss=0.684972356926995\n",
      "Current iteration=800, loss=0.6782007843835163\n",
      "Current iteration=900, loss=0.6718481210720425\n",
      "training_loss = 0.6658822265840392, validation_loss = 0.4942336560255588\n",
      "Current gamma=0.0031622776601683794\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.6814876233539293\n",
      "Current iteration=200, loss=0.6408545749220007\n",
      "Current iteration=300, loss=0.6143297827907644\n",
      "Current iteration=400, loss=0.5960738273801767\n",
      "Current iteration=500, loss=0.5829453860406754\n",
      "Current iteration=600, loss=0.5731512478560261\n",
      "Current iteration=700, loss=0.5656159497619946\n",
      "Current iteration=800, loss=0.5596673822401969\n",
      "Current iteration=900, loss=0.5548687573355013\n",
      "training_loss = 0.5509258561065727, validation_loss = 0.4723157238721199\n",
      "Current gamma=0.023713737056616554\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.5622914649514105\n",
      "Current iteration=200, loss=0.5385188351826407\n",
      "Current iteration=300, loss=0.5298263950258527\n",
      "Current iteration=400, loss=0.525112788769284\n",
      "Current iteration=500, loss=0.5219760396898797\n",
      "Current iteration=600, loss=0.5196370848148032\n",
      "Current iteration=700, loss=0.5177745977737381\n",
      "Current iteration=800, loss=0.5162309986407689\n",
      "Current iteration=900, loss=0.5149181128955009\n",
      "training_loss = 0.5137813478468388, validation_loss = 0.4970378122086174\n",
      "Current gamma=0.1778279410038923\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.5169326305534832\n",
      "Current iteration=200, loss=0.5097457456140237\n",
      "Current iteration=300, loss=0.5064102222818608\n",
      "Current iteration=400, loss=0.5044995480902223\n",
      "Current iteration=500, loss=0.5032840109946595\n",
      "Current iteration=600, loss=0.5024653798786597\n",
      "Current iteration=700, loss=0.5018941076789492\n",
      "Current iteration=800, loss=0.5014860341754573\n",
      "Current iteration=900, loss=0.501189837593079\n",
      "training_loss = 0.5009723695513127, validation_loss = 0.5533130173627839\n",
      "Current gamma=1.333521432163324\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=0.5016513602270423\n",
      "Current iteration=200, loss=0.5004785351410553\n",
      "Current iteration=300, loss=0.5003397477716163\n",
      "Current iteration=400, loss=0.5003208600624562\n",
      "breaking threshold\n",
      "training_loss = 0.5003180902605044, validation_loss = 0.5660388570630032\n",
      "Current gamma=10.0\n",
      "Current iteration=0, loss=0.7473898389524194\n",
      "Current iteration=100, loss=1.5321923765878052\n",
      "Current iteration=200, loss=1.6092667306511519\n",
      "Current iteration=300, loss=1.6089665768222428\n",
      "Current iteration=400, loss=1.608617165790676\n",
      "Current iteration=500, loss=1.6083506631829059\n",
      "Current iteration=600, loss=1.608150826323132\n",
      "Current iteration=700, loss=1.6080039131365338\n",
      "Current iteration=800, loss=1.6078982262436157\n",
      "Current iteration=900, loss=1.607823538384268\n",
      "training_loss = 1.6077714049349616, validation_loss = 1.804409195129949\n",
      "Best gamma = 0.0031622776601683794, training_loss = 0.5509258561065727, validation_loss = 0.4723157238721199\n",
      "Current iteration=0, loss=0.7164092842913496\n",
      "Current iteration=100, loss=0.6744685848750503\n",
      "Current iteration=200, loss=0.6464290551532166\n",
      "Current iteration=300, loss=0.6261264314100615\n",
      "Current iteration=400, loss=0.6106151455440434\n",
      "Current iteration=500, loss=0.5983278440909451\n",
      "Current iteration=600, loss=0.5883450447727487\n",
      "Current iteration=700, loss=0.5800824844421905\n",
      "Current iteration=800, loss=0.5731449131523574\n",
      "Current iteration=900, loss=0.5672518921778836\n"
     ]
    }
   ],
   "source": [
    "#Find the value of gamma that minimizes the loss:\n",
    "gamma_opt = hpopt.best_gamma_selection(y_train, tx_train, 1000)\n",
    "# Train model\n",
    "trained_weights, train_loss = impl.logistic_regression(y_train, tx_train, initial_w, max_iters=1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy = 0.7263573333333333\n",
      "validation_accuracy = 0.727312\n"
     ]
    }
   ],
   "source": [
    "# Cross validation\n",
    "predict_validation = helpers.predict_logistic(tx_validation, trained_weights)\n",
    "predict_train = helpers.predict_logistic(tx_train, trained_weights)\n",
    "\n",
    "predict_validation[predict_validation == -1] = 0\n",
    "predict_train[predict_train == -1] = 0\n",
    "\n",
    "train_accuracy = helpers.accuracy(predict_train, y_train)\n",
    "validation_accuracy = helpers.accuracy(predict_validation, y_validation)\n",
    "\n",
    "print(f\"train_accuracy = {train_accuracy}\")\n",
    "print(f\"validation_accuracy = {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Logistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression with polynomial features:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.7134303278406444\n",
      "Current iteration=100, loss=0.7038088924337598\n",
      "Current iteration=200, loss=0.6955266212239213\n",
      "Current iteration=300, loss=0.6883982334777878\n",
      "Current iteration=400, loss=0.6822628318628193\n",
      "Current iteration=500, loss=0.6769812224565813\n",
      "Current iteration=600, loss=0.6724333364069965\n",
      "Current iteration=700, loss=0.6685158271684682\n",
      "Current iteration=800, loss=0.6651398817597853\n",
      "Current iteration=900, loss=0.6622292601166271\n",
      "Current iteration=0, loss=0.6821858949373834\n",
      "Current iteration=100, loss=0.6767444908119747\n",
      "Current iteration=200, loss=0.6720590033966609\n",
      "Current iteration=300, loss=0.6680229347964532\n",
      "Current iteration=400, loss=0.6645447631095672\n",
      "Current iteration=500, loss=0.6615459254931555\n",
      "Current iteration=600, loss=0.6589590320819722\n",
      "Current iteration=700, loss=0.656726299868922\n",
      "Current iteration=800, loss=0.6547981897204165\n",
      "Current iteration=900, loss=0.6531322269199542\n",
      "Current iteration=0, loss=0.6901601098315059\n",
      "Current iteration=100, loss=0.6836894493599057\n",
      "Current iteration=200, loss=0.6781195364027087\n",
      "Current iteration=300, loss=0.6733237713728861\n",
      "Current iteration=400, loss=0.6691931293130531\n",
      "Current iteration=500, loss=0.6656338841498306\n",
      "Current iteration=600, loss=0.662565560667164\n",
      "Current iteration=700, loss=0.6599191166018556\n",
      "Current iteration=800, loss=0.6576353456905774\n",
      "Current iteration=900, loss=0.6556634858533177\n",
      "Current iteration=0, loss=0.689495945988957\n",
      "Current iteration=100, loss=0.6830615957676928\n",
      "Current iteration=200, loss=0.677522827609221\n",
      "Current iteration=300, loss=0.672753727098986\n",
      "Current iteration=400, loss=0.6686458790378156\n",
      "Current iteration=500, loss=0.6651060948727178\n",
      "Current iteration=600, loss=0.662054369700474\n",
      "Current iteration=700, loss=0.6594220703390945\n",
      "Current iteration=800, loss=0.6571503446792567\n",
      "Current iteration=900, loss=0.6551887360997027\n",
      "Current iteration=0, loss=0.6867120710407606\n",
      "Current iteration=100, loss=0.6491040031644065\n",
      "Current iteration=200, loss=0.6242342298012683\n",
      "Current iteration=300, loss=0.606578387460035\n",
      "Current iteration=400, loss=0.5933725599866384\n",
      "Current iteration=500, loss=0.5831238103816286\n",
      "Current iteration=600, loss=0.5749547359761076\n",
      "Current iteration=700, loss=0.5683109527293712\n",
      "Current iteration=800, loss=0.5628199076499724\n",
      "Current iteration=900, loss=0.5582184754050701\n",
      "Current iteration=0, loss=0.6890681780723876\n",
      "Current iteration=100, loss=0.6566275313149851\n",
      "Current iteration=200, loss=0.6327308712597346\n",
      "Current iteration=300, loss=0.6146389294625388\n",
      "Current iteration=400, loss=0.6006224308358761\n",
      "Current iteration=500, loss=0.5895493896901065\n",
      "Current iteration=600, loss=0.5806535220219801\n",
      "Current iteration=700, loss=0.5734006564170219\n",
      "Current iteration=800, loss=0.5674090992702521\n",
      "Current iteration=900, loss=0.5624004490639943\n",
      "Current iteration=0, loss=0.7159411877252665\n",
      "Current iteration=100, loss=0.6677267415546678\n",
      "Current iteration=200, loss=0.6354740126521801\n",
      "Current iteration=300, loss=0.6128131710997363\n",
      "Current iteration=400, loss=0.5962461606895456\n",
      "Current iteration=500, loss=0.5837489044656439\n",
      "Current iteration=600, loss=0.5740828321393342\n",
      "Current iteration=700, loss=0.5664514385798284\n",
      "Current iteration=800, loss=0.5603212039320566\n",
      "Current iteration=900, loss=0.555322636744944\n",
      "Current iteration=0, loss=0.7805090171592648\n",
      "Current iteration=100, loss=0.7183627990484353\n",
      "Current iteration=200, loss=0.6735537516774923\n",
      "Current iteration=300, loss=0.640880916677094\n",
      "Current iteration=400, loss=0.616710756151599\n",
      "Current iteration=500, loss=0.5985446394594107\n",
      "Current iteration=600, loss=0.5846700500721076\n",
      "Current iteration=700, loss=0.5739043376845827\n",
      "Current iteration=800, loss=0.5654218353102362\n",
      "Current iteration=900, loss=0.5586405585778547\n",
      "Current iteration=0, loss=0.7856237792860403\n",
      "Current iteration=100, loss=0.6550810608150991\n",
      "Current iteration=200, loss=0.6119285879021611\n",
      "Current iteration=300, loss=0.5864941068730597\n",
      "Current iteration=400, loss=0.5682872525162761\n",
      "Current iteration=500, loss=0.5540756941111672\n",
      "Current iteration=600, loss=0.5425436745436321\n",
      "Current iteration=700, loss=0.5329911470787779\n",
      "Current iteration=800, loss=0.5249303396350837\n",
      "Current iteration=900, loss=0.5180284680997861\n",
      "Current iteration=0, loss=0.9764156478693766\n",
      "Current iteration=100, loss=0.6498277212558867\n",
      "Current iteration=200, loss=0.58039996102807\n",
      "Current iteration=300, loss=0.5546957817859677\n",
      "Current iteration=400, loss=0.5397361764378305\n",
      "Current iteration=500, loss=0.5288761811491399\n",
      "Current iteration=600, loss=0.5203234566787839\n",
      "Current iteration=700, loss=0.5133341231558183\n",
      "Current iteration=800, loss=0.507471138785787\n",
      "Current iteration=900, loss=0.5024481402524678\n",
      "Current iteration=0, loss=0.8952591851326128\n",
      "Current iteration=100, loss=0.6360655695995392\n",
      "Current iteration=200, loss=0.5874482976611427\n",
      "Current iteration=300, loss=0.5653394596027689\n",
      "Current iteration=400, loss=0.5499417356206748\n",
      "Current iteration=500, loss=0.5378605953261018\n",
      "Current iteration=600, loss=0.5279424871730612\n",
      "Current iteration=700, loss=0.5196186594389295\n",
      "Current iteration=800, loss=0.5125447283798209\n",
      "Current iteration=900, loss=0.5064834145045207\n",
      "Current iteration=0, loss=0.9112038616114126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=0.6432669368537081\n",
      "Current iteration=200, loss=0.5807195350196399\n",
      "Current iteration=300, loss=0.5575282307207049\n",
      "Current iteration=400, loss=0.542708086681093\n",
      "Current iteration=500, loss=0.5315256148032612\n",
      "Current iteration=600, loss=0.5225995836671046\n",
      "Current iteration=700, loss=0.5152607414736856\n",
      "Current iteration=800, loss=0.509107187978798\n",
      "Current iteration=900, loss=0.5038687979692179\n",
      "Current iteration=0, loss=2.2759750033948687\n",
      "Current iteration=100, loss=0.6752075428394545\n",
      "Current iteration=200, loss=0.5949050736157269\n",
      "Current iteration=300, loss=0.5542592376956457\n",
      "Current iteration=400, loss=0.5384122151849594\n",
      "Current iteration=500, loss=0.5268864816771814\n",
      "Current iteration=600, loss=0.517736730629972\n",
      "Current iteration=700, loss=0.5101829997370434\n",
      "Current iteration=800, loss=0.5037439181019883\n",
      "Current iteration=900, loss=0.4982305385945275\n",
      "Current iteration=0, loss=2.274976829081178\n",
      "Current iteration=100, loss=0.6983884206805918\n",
      "Current iteration=200, loss=0.5989532664323411\n",
      "Current iteration=300, loss=0.5659144341555749\n",
      "Current iteration=400, loss=0.5474406807628841\n",
      "Current iteration=500, loss=0.5338977817323565\n",
      "Current iteration=600, loss=0.5232543790476064\n",
      "Current iteration=700, loss=0.5149800340034314\n",
      "Current iteration=800, loss=0.5077012011556756\n",
      "Current iteration=900, loss=0.50145578692779\n",
      "Current iteration=0, loss=2.6119899872828998\n",
      "Current iteration=100, loss=0.6784870393626098\n",
      "Current iteration=200, loss=0.6085201607424919\n",
      "Current iteration=300, loss=0.5742940896050018\n",
      "Current iteration=400, loss=0.5519676249814599\n",
      "Current iteration=500, loss=0.5358644617507821\n",
      "Current iteration=600, loss=0.5236343100228965\n",
      "Current iteration=700, loss=0.5139973240508359\n",
      "Current iteration=800, loss=0.5061703892513377\n",
      "Current iteration=900, loss=0.49965833207332017\n",
      "Current iteration=0, loss=1.7354253770808006\n",
      "Current iteration=100, loss=0.6902726132043631\n",
      "Current iteration=200, loss=0.6061411091543644\n",
      "Current iteration=300, loss=0.5714097955728612\n",
      "Current iteration=400, loss=0.5503420684414203\n",
      "Current iteration=500, loss=0.535393270896002\n",
      "Current iteration=600, loss=0.5240522370204439\n",
      "Current iteration=700, loss=0.5152763981723061\n",
      "Current iteration=800, loss=0.5086885188284975\n",
      "Current iteration=900, loss=0.5016755810093698\n",
      "Current iteration=0, loss=109.7716936794767\n",
      "Current iteration=100, loss=8.92405473008204\n",
      "Current iteration=200, loss=3.5360168770319036\n",
      "Current iteration=300, loss=1.2370068447832514\n",
      "Current iteration=400, loss=1.135680297961361\n",
      "Current iteration=500, loss=1.7498044442353118\n",
      "Current iteration=600, loss=15.959210090945072\n",
      "Current iteration=700, loss=2.4414224308660564\n",
      "Current iteration=800, loss=1.4323258419504803\n",
      "Current iteration=900, loss=1.209362112201021\n",
      "Current iteration=0, loss=242.20588674843665\n",
      "Current iteration=100, loss=8.353619280427274\n",
      "Current iteration=200, loss=1.4746257140020915\n",
      "Current iteration=300, loss=1.1007842301038173\n",
      "Current iteration=400, loss=1.5507109770501657\n",
      "Current iteration=500, loss=1.4665353115206297\n",
      "Current iteration=600, loss=1.0372537968547484\n",
      "Current iteration=700, loss=0.9252510817609899\n",
      "Current iteration=800, loss=22.108845277499984\n",
      "Current iteration=900, loss=1.4822013845929183\n",
      "Current iteration=0, loss=10.149705625416988\n",
      "Current iteration=100, loss=2.073137560907154\n",
      "Current iteration=200, loss=2.190578385377597\n",
      "Current iteration=300, loss=1.4354922079056145\n",
      "Current iteration=400, loss=1.1934802344562265\n",
      "Current iteration=500, loss=1.2877548130602052\n",
      "Current iteration=600, loss=1.2099762282372397\n",
      "Current iteration=700, loss=3.842478231319358\n",
      "Current iteration=800, loss=1.0698342153348723\n",
      "Current iteration=900, loss=1.9644165869925982\n",
      "Current iteration=0, loss=19.200875042540908\n",
      "Current iteration=100, loss=11.841385379769255\n",
      "Current iteration=200, loss=2.308709135625179\n",
      "Current iteration=300, loss=3.4189832921236274\n",
      "Current iteration=400, loss=1.6818319728449862\n",
      "Current iteration=500, loss=1.3039677968717964\n",
      "Current iteration=600, loss=21.477459072473795\n",
      "Current iteration=700, loss=1.6595589001260953\n",
      "Current iteration=800, loss=2.234312941984731\n",
      "Current iteration=900, loss=1.3945898794881884\n"
     ]
    }
   ],
   "source": [
    "#Use cross validation to find the best degree (prevent overfitting):\n",
    "degree_opt = cv.best_degree_selection_logistic(train_labels, new_data, 1000, 0.003, np.arange(0,5,1), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 125)\n"
     ]
    }
   ],
   "source": [
    "#Build polynomial with the optimal degree found previously:\n",
    "train_poly = cv.build_poly(new_data, 4)\n",
    "print(train_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125,)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize weight vector such that the dimensions are adapted to the polynomial feature matrix:\n",
    "initial_w = np.random.normal(0., 0.1, [train_poly.shape[1],])\n",
    "print(initial_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=7.626934296379402\n",
      "Current iteration=100, loss=4.360285865953201\n",
      "Current iteration=200, loss=1.3865786653010366\n",
      "Current iteration=300, loss=2.4064495870187135\n",
      "Current iteration=400, loss=15.73407291954145\n",
      "Current iteration=500, loss=1.7192420896345593\n",
      "Current iteration=600, loss=2.263713930892244\n",
      "Current iteration=700, loss=1.3609727607266202\n",
      "Current iteration=800, loss=1.321732455500975\n",
      "Current iteration=900, loss=3.6631454303775137\n"
     ]
    }
   ],
   "source": [
    "#Train the model with polynomial features:\n",
    "trained_weights_poly, train_loss_poly = impl.logistic_regression(train_labels, train_poly, initial_w, max_iters=1000, gamma=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 125)\n",
      "(125,)\n"
     ]
    }
   ],
   "source": [
    "#Build polynomial basis from test data:\n",
    "test_poly = cv.build_poly(tx_test, 4)\n",
    "print(test_poly.shape)\n",
    "print(trained_weights_poly.shape)\n",
    "#Compute predicted labels:\n",
    "predicted_labels = helpers.predict_logistic(test_poly, trained_weights_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predicted_labels, 'Predictions_Logistics_Polynomial.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [new_data.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current gamma=1e-06\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=63.90647630054176\n",
      "Current iteration=200, loss=53.02343254839472\n",
      "Current iteration=300, loss=44.368158332960675\n",
      "Current iteration=400, loss=35.82468729084078\n",
      "Current iteration=500, loss=27.30095531012157\n",
      "Current iteration=600, loss=18.81347756457517\n",
      "Current iteration=700, loss=11.95380385074749\n",
      "Current iteration=800, loss=9.503195366473074\n",
      "Current iteration=900, loss=7.751785287349152\n",
      "training_loss = 6.1669070611112256, validation_loss = 9.739648296571119\n",
      "Current gamma=7.498942093324558e-06\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=10.509610190754243\n",
      "Current iteration=200, loss=3.701115962311486\n",
      "Current iteration=300, loss=3.7455438017163165\n",
      "Current iteration=400, loss=3.670723311723105\n",
      "Current iteration=500, loss=3.653082424181691\n",
      "Current iteration=600, loss=3.652305190781663\n",
      "Current iteration=700, loss=3.3775144486288284\n",
      "Current iteration=800, loss=2.507512000503006\n",
      "Current iteration=900, loss=2.8897558963186927\n",
      "training_loss = 3.047229588142272, validation_loss = 4.466476361433559\n",
      "Current gamma=5.623413251903491e-05\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=31.876324038864727\n",
      "Current iteration=200, loss=4.113117108343962\n",
      "Current iteration=300, loss=42.344710961842445\n",
      "Current iteration=400, loss=30.63807277176464\n",
      "Current iteration=500, loss=2.534810177606366\n",
      "Current iteration=600, loss=41.47272218653498\n",
      "Current iteration=700, loss=32.26595899426802\n",
      "Current iteration=800, loss=1.8494362293077253\n",
      "Current iteration=900, loss=41.76642142549732\n",
      "training_loss = 40.76847827329069, validation_loss = -36.892696911982256\n",
      "Current gamma=0.00042169650342858224\n",
      "Current iteration=0, loss=74.99274837100322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=288.5998504558666\n",
      "Current iteration=200, loss=277.4259364801881\n",
      "Current iteration=300, loss=311.6132322804477\n",
      "Current iteration=400, loss=233.80921590042038\n",
      "Current iteration=500, loss=130.36664938852536\n",
      "Current iteration=600, loss=340.08096962468056\n",
      "Current iteration=700, loss=352.2575230792863\n",
      "Current iteration=800, loss=257.32160121572747\n",
      "Current iteration=900, loss=230.92136708567338\n",
      "training_loss = 133.61811692541337, validation_loss = -132.94876036141915\n",
      "Current gamma=0.0031622776601683794\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=1803.5479482008486\n",
      "Current iteration=200, loss=2698.8193117157975\n",
      "Current iteration=300, loss=1850.6770403982462\n",
      "Current iteration=400, loss=2333.647888915868\n",
      "Current iteration=500, loss=878.2525509373368\n",
      "Current iteration=600, loss=1606.302304471584\n",
      "Current iteration=700, loss=1617.6304754495534\n",
      "Current iteration=800, loss=908.8648642171975\n",
      "Current iteration=900, loss=120.91549526963401\n",
      "training_loss = 2758.6421440668587, validation_loss = -2536.5218485894447\n",
      "Current gamma=0.023713737056616554\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=15403.224942138017\n",
      "Current iteration=200, loss=824.7425400884929\n",
      "Current iteration=300, loss=12527.164871123743\n",
      "Current iteration=400, loss=3121.6023145254735\n",
      "Current iteration=500, loss=5947.607336791023\n",
      "Current iteration=600, loss=16341.972192923815\n",
      "Current iteration=700, loss=11379.5018646704\n",
      "Current iteration=800, loss=1652.2421592002208\n",
      "Current iteration=900, loss=14915.067794436834\n",
      "training_loss = 16706.14125504352, validation_loss = -15265.15569367416\n",
      "Current gamma=0.1778279410038923\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=137587.45494151118\n",
      "Current iteration=200, loss=138130.96678896947\n",
      "Current iteration=300, loss=131718.41479861824\n",
      "Current iteration=400, loss=156146.51882894\n",
      "Current iteration=500, loss=89175.00948583528\n",
      "Current iteration=600, loss=53774.58585608675\n",
      "Current iteration=700, loss=128784.8463804457\n",
      "Current iteration=800, loss=100657.69285440666\n",
      "Current iteration=900, loss=134318.60062260914\n",
      "training_loss = 102781.73493417678, validation_loss = -92708.00053273117\n",
      "Current gamma=1.333521432163324\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=1031734.3599336548\n",
      "Current iteration=200, loss=1111806.4231106485\n",
      "Current iteration=300, loss=344885.14915403863\n",
      "Current iteration=400, loss=321037.8646206368\n",
      "Current iteration=500, loss=62798.202864400504\n",
      "Current iteration=600, loss=48938.80712875768\n",
      "Current iteration=700, loss=122740.9170284914\n",
      "Current iteration=800, loss=975748.2190812235\n",
      "Current iteration=900, loss=639489.6580215346\n",
      "training_loss = 308268.0985015996, validation_loss = -306582.6430895334\n",
      "Current gamma=10.0\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=7741514.932327581\n",
      "Current iteration=200, loss=5894682.516363727\n",
      "Current iteration=300, loss=8313801.210662053\n",
      "Current iteration=400, loss=5397785.537483745\n",
      "Current iteration=500, loss=7939516.106133644\n",
      "Current iteration=600, loss=6184868.5579700135\n",
      "Current iteration=700, loss=8145501.639071651\n",
      "Current iteration=800, loss=7397109.395408013\n",
      "Current iteration=900, loss=3357158.489083121\n",
      "training_loss = 7821287.60587992, validation_loss = 15534640.376151886\n",
      "Best gamma = 1.333521432163324, training_loss = 308268.0985015996, validation_loss = -306582.6430895334\n",
      "Current lambda=1e-06\n",
      "Current it = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 297314.1500673294, validation_loss = -295925.23013583047\n",
      "Current lambda=0.00031622776601683794\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 229577.1774475597, validation_loss = -228541.58335607703\n",
      "Current lambda=0.1\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 929082.2731807932, validation_loss = -803379.3659650225\n",
      "Best lambda = 0.1, training_loss = 929082.2731807932, validation_loss = -803379.3659650225\n"
     ]
    }
   ],
   "source": [
    "#Find the most optimal values for the regularization term (lambda) and gamma:\n",
    "gamma_opt = hpopt.best_gamma_selection(train_labels, train_data, 1000)\n",
    "lambda_opt = hpopt.best_lambda_selection(train_labels, train_data, 1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n"
     ]
    }
   ],
   "source": [
    "trained_weights, train_loss = impl.reg_logistic_regression(train_labels, new_data, lambda_opt, initial_w, max_iters=1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ...  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), tx_test]\n",
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_RegLogistics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
