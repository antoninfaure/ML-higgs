{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0118fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "tx_train, y_train, ids_train = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e5e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor the -1 in 0 value for logistic regression\n",
    "y_train[y_train==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d433f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "y_train, tx_train = helpers.shuffle_data(y_train, tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8242a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean data into 4 sets according to 22nd feature\n",
    "tx_train_0, y_0, _, miss_col_0 = helpers.split_i(tx_train, y_train, ids_train, 0)\n",
    "tx_train_1, y_1, _, miss_col_1 = helpers.split_i(tx_train, y_train, ids_train, 1)\n",
    "tx_train_2, y_2, _, miss_col_2 = helpers.split_i(tx_train, y_train, ids_train, 2)\n",
    "tx_train_3, y_3, _, miss_col_3 = helpers.split_i(tx_train, y_train, ids_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e7ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "tx_train_0, mean_0, std_0 = helpers.standardize(tx_train_0)\n",
    "tx_train_1, mean_1, std_1 = helpers.standardize(tx_train_1)\n",
    "tx_train_2, mean_2, std_2 = helpers.standardize(tx_train_2)\n",
    "tx_train_3, mean_3, std_3 = helpers.standardize(tx_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94041bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to degree 2\n",
    "tx_train_0 = helpers.build_poly_deg2(tx_train_0)\n",
    "tx_train_1 = helpers.build_poly_deg2(tx_train_1)\n",
    "tx_train_2 = helpers.build_poly_deg2(tx_train_2)\n",
    "tx_train_3 = helpers.build_poly_deg2(tx_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a12bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_train_0 = np.c_[np.ones((tx_train_0.shape[0], 1)), tx_train_0]\n",
    "tx_train_1 = np.c_[np.ones((tx_train_1.shape[0], 1)), tx_train_1]\n",
    "tx_train_2 = np.c_[np.ones((tx_train_2.shape[0], 1)), tx_train_2]\n",
    "tx_train_3 = np.c_[np.ones((tx_train_3.shape[0], 1)), tx_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ecf9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 190)\n",
      "(77544, 276)\n",
      "(50379, 465)\n",
      "(22164, 465)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train_0.shape)\n",
    "print(tx_train_1.shape)\n",
    "print(tx_train_2.shape)\n",
    "print(tx_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8ff64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  5  6 12 23 24 25 26 27 28 22 29]\n",
      "[ 4  5  6 12 26 27 28 22]\n",
      "[22]\n",
      "[22]\n"
     ]
    }
   ],
   "source": [
    "print(miss_col_0)\n",
    "print(miss_col_1)\n",
    "print(miss_col_2)\n",
    "print(miss_col_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30513e86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.930253206858558\n",
      "Current iteration=100, loss=0.382231648815959\n",
      "Current iteration=200, loss=0.37158384357855984\n",
      "Current iteration=300, loss=0.3675310408997164\n",
      "Current iteration=400, loss=0.36522891670223256\n",
      "Current iteration=500, loss=0.3636809580723967\n",
      "Current iteration=600, loss=0.36259163333904465\n",
      "Current iteration=700, loss=0.361790686094909\n",
      "Current iteration=800, loss=0.3611685286083176\n",
      "Current iteration=900, loss=0.3607287349906466\n",
      "Current iteration=1000, loss=0.3602406415502376\n",
      "Current iteration=1100, loss=0.3598907625569575\n",
      "Current iteration=1200, loss=0.3596794877141411\n",
      "Current iteration=1300, loss=0.3593247356949532\n",
      "Current iteration=1400, loss=0.35910474867285186\n",
      "Current iteration=1500, loss=0.3590077399336714\n",
      "Current iteration=1600, loss=0.3587252034891172\n",
      "Current iteration=1700, loss=0.3585795133420588\n",
      "Current iteration=1800, loss=0.35854627005205375\n",
      "Current iteration=1900, loss=0.3583070581577359\n",
      "Current iteration=2000, loss=0.3582075242080674\n",
      "Current iteration=2100, loss=0.35821282082504896\n",
      "Current iteration=2200, loss=0.35800136934023563\n",
      "Current iteration=2300, loss=0.3579325254691908\n",
      "Current iteration=2400, loss=0.35796268639701584\n",
      "Current iteration=2500, loss=0.35777034881480246\n",
      "Current iteration=2600, loss=0.3577227015712536\n",
      "Current iteration=2700, loss=0.3577699279392248\n",
      "Current iteration=2800, loss=0.3575914328571427\n",
      "Current iteration=2900, loss=0.3575586743662019\n",
      "Current iteration=3000, loss=0.3576182823107759\n",
      "Current iteration=3100, loss=0.3574501735244207\n",
      "Current iteration=3200, loss=0.3574279565457377\n",
      "Current iteration=3300, loss=0.35749694959505074\n",
      "Current iteration=3400, loss=0.35733685131689275\n",
      "Current iteration=3500, loss=0.3573221508996204\n",
      "Current iteration=3600, loss=0.3573984682875938\n",
      "Current iteration=3700, loss=0.35724469332391046\n",
      "Current iteration=3800, loss=0.35723540846213464\n",
      "Current iteration=3900, loss=0.3573175365194353\n",
      "Current iteration=4000, loss=0.3571688541863418\n",
      "Current iteration=4100, loss=0.35716353030824005\n",
      "Current iteration=4200, loss=0.3572503045856955\n",
      "Current iteration=4300, loss=0.3571057942217364\n",
      "Current iteration=4400, loss=0.3571034229753768\n",
      "Current iteration=4500, loss=0.35719392510286857\n",
      "Current iteration=4600, loss=0.3570528810200567\n",
      "Current iteration=4700, loss=0.3570527573260249\n",
      "Current iteration=4800, loss=0.3571462559671574\n",
      "Current iteration=4900, loss=0.3570081250937314\n",
      "Current iteration=5000, loss=0.3570097480122837\n",
      "Current iteration=5100, loss=0.3571056596518265\n",
      "Current iteration=5200, loss=0.3569700001322029\n",
      "Current iteration=5300, loss=0.35697300616251293\n",
      "Current iteration=5400, loss=0.3570708655061834\n",
      "Current iteration=5500, loss=0.3569373186480947\n",
      "Current iteration=5600, loss=0.356941437816151\n",
      "Current iteration=5700, loss=0.3570408740134797\n",
      "Current iteration=5800, loss=0.3569091447611838\n",
      "Current iteration=5900, loss=0.35691417197430464\n",
      "Current iteration=6000, loss=0.3570148892801491\n",
      "Current iteration=6100, loss=0.35688473224097356\n",
      "Current iteration=6200, loss=0.3568905085227471\n",
      "Current iteration=6300, loss=0.3569922706940262\n",
      "Current iteration=6400, loss=0.3568634798893473\n",
      "Current iteration=6500, loss=0.3568698798913213\n",
      "Current iteration=6600, loss=0.3569724977410284\n",
      "Current iteration=6700, loss=0.3568448989294454\n",
      "Current iteration=6800, loss=0.3568518224200047\n",
      "Current iteration=6900, loss=0.35695514397611094\n",
      "Current iteration=7000, loss=0.3568285887966997\n",
      "Current iteration=7100, loss=0.35683595469934853\n",
      "Current iteration=7200, loss=0.35693985746217616\n",
      "Current iteration=7300, loss=0.3568142188907538\n",
      "Current iteration=7400, loss=0.3568219609930465\n",
      "Current iteration=7500, loss=0.35692634585562794\n",
      "Current iteration=7600, loss=0.35680151462418486\n",
      "Current iteration=7700, loss=0.3568095784124675\n",
      "Current iteration=7800, loss=0.35691436488646355\n",
      "Current iteration=7900, loss=0.35679024662063774\n",
      "Current iteration=8000, loss=0.35679858689606553\n",
      "Current iteration=8100, loss=0.3569037093580494\n",
      "Current iteration=8200, loss=0.3567802222588174\n",
      "Current iteration=8300, loss=0.35678880131071716\n",
      "Current iteration=8400, loss=0.35689420604474137\n",
      "Current iteration=8500, loss=0.3567712789895547\n",
      "Current iteration=8600, loss=0.3567800651760912\n",
      "Current iteration=8700, loss=0.35688570803781755\n",
      "Current iteration=8800, loss=0.35676327901034816\n",
      "Current iteration=8900, loss=0.3567722456429204\n",
      "Current iteration=9000, loss=0.35687809020951905\n",
      "Current iteration=9100, loss=0.35675610499080085\n",
      "Current iteration=9200, loss=0.3567652294487325\n",
      "Current iteration=9300, loss=0.35687124554911304\n",
      "Current iteration=9400, loss=0.3567496566194381\n",
      "Current iteration=9500, loss=0.35675891964168316\n",
      "Current iteration=9600, loss=0.35686508218516294\n",
      "Current iteration=9700, loss=0.35674384779788\n",
      "Current iteration=9800, loss=0.35675323291234223\n",
      "Current iteration=9900, loss=0.356859520952077\n",
      "Current iteration=10000, loss=0.3567386043489482\n",
      "Current iteration=10100, loss=0.3567480974098078\n",
      "Current iteration=10200, loss=0.3568544933913965\n",
      "Current iteration=10300, loss=0.35673386213543956\n",
      "Current iteration=10400, loss=0.3567434509459456\n",
      "Current iteration=10500, loss=0.35684994010252546\n",
      "Current iteration=10600, loss=0.35672956550897095\n",
      "Current iteration=10700, loss=0.3567392395123473\n",
      "Current iteration=10800, loss=0.35684580937592636\n",
      "Current iteration=10900, loss=0.3567256660255284\n",
      "Current iteration=11000, loss=0.35673541605050546\n",
      "Current iteration=11100, loss=0.3568420560558202\n",
      "Current iteration=11200, loss=0.35672212137757364\n",
      "Current iteration=11300, loss=0.35673193942797615\n",
      "Current iteration=11400, loss=0.3568386405902416\n",
      "Current iteration=11500, loss=0.35671889450278693\n",
      "Current iteration=11600, loss=0.356728773582839\n",
      "Current iteration=11700, loss=0.3568355282346985\n",
      "Current iteration=11800, loss=0.35671595283749424\n",
      "Current iteration=11900, loss=0.3567258868062113\n",
      "Current iteration=12000, loss=0.35683268838227755\n",
      "Current iteration=12100, loss=0.35671326768907746\n",
      "Current iteration=12200, loss=0.35672325113845527\n",
      "Current iteration=12300, loss=0.3568300939982334\n",
      "Current iteration=12400, loss=0.3567108137066044\n",
      "Current iteration=12500, loss=0.3567208418593487\n",
      "Current iteration=12600, loss=0.35682772114121886\n",
      "Current iteration=12700, loss=0.3567085684328231\n",
      "Current iteration=12800, loss=0.35671863705619133\n",
      "Current iteration=12900, loss=0.35682554855660675\n",
      "Current iteration=13000, loss=0.35670651192379604\n",
      "Current iteration=13100, loss=0.35671661725676906\n",
      "Current iteration=13200, loss=0.35682355732998966\n",
      "Current iteration=13300, loss=0.3567046264249446\n",
      "Current iteration=13400, loss=0.3567147651164635\n",
      "Current iteration=13500, loss=0.35682173059107075\n",
      "Current iteration=13600, loss=0.3567028960942932\n",
      "Current iteration=13700, loss=0.356713065150713\n",
      "Current iteration=13800, loss=0.3568200532598778\n",
      "Current iteration=13900, loss=0.3567013067653232\n",
      "Current iteration=14000, loss=0.3567115035055683\n",
      "Current iteration=14100, loss=0.3568185118286271\n",
      "Current iteration=14200, loss=0.35669984574317504\n",
      "Current iteration=14300, loss=0.3567100677603464\n",
      "Current iteration=14400, loss=0.3568170941737027\n",
      "Current iteration=14500, loss=0.35669850162899985\n",
      "Current iteration=14600, loss=0.3567087467574072\n",
      "Current iteration=14700, loss=0.35681578939314745\n",
      "Current iteration=14800, loss=0.3566972641681489\n",
      "Current iteration=14900, loss=0.3567075304549162\n",
      "Current iteration=0, loss=0.9060423125405851\n",
      "Current iteration=100, loss=0.6355789434797375\n",
      "Current iteration=200, loss=0.5712123567935007\n",
      "Current iteration=300, loss=0.5363378890179424\n",
      "Current iteration=400, loss=0.514859540384605\n",
      "Current iteration=500, loss=0.5005688652157035\n",
      "Current iteration=600, loss=0.49053822656809554\n",
      "Current iteration=700, loss=0.4832095374703819\n",
      "Current iteration=800, loss=0.4776851707053599\n",
      "Current iteration=900, loss=0.4734100450485626\n",
      "Current iteration=1000, loss=0.47002616175164014\n",
      "Current iteration=1100, loss=0.46729673082970097\n",
      "Current iteration=1200, loss=0.465060150855726\n",
      "Current iteration=1300, loss=0.4632024782362539\n",
      "Current iteration=1400, loss=0.46164104279935286\n",
      "Current iteration=1500, loss=0.4603144529519092\n",
      "Current iteration=1600, loss=0.4591762763846087\n",
      "Current iteration=1700, loss=0.4581908838629068\n",
      "Current iteration=1800, loss=0.45733060662702973\n",
      "Current iteration=1900, loss=0.45657372878397096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2000, loss=0.45590303776235325\n",
      "Current iteration=2100, loss=0.4553047625405546\n",
      "Current iteration=2200, loss=0.45476778781629074\n",
      "Current iteration=2300, loss=0.45428306718982636\n",
      "Current iteration=2400, loss=0.45384318106739274\n",
      "Current iteration=2500, loss=0.45344200049436206\n",
      "Current iteration=2600, loss=0.45307442906011614\n",
      "Current iteration=2700, loss=0.4527362028150259\n",
      "Current iteration=2800, loss=0.45242373370720596\n",
      "Current iteration=2900, loss=0.45213398600724947\n",
      "Current iteration=3000, loss=0.4518643780000884\n",
      "Current iteration=3100, loss=0.4516127032222897\n",
      "Current iteration=3200, loss=0.45137706695507634\n",
      "Current iteration=3300, loss=0.45115583472038906\n",
      "Current iteration=3400, loss=0.45094759028801\n",
      "Current iteration=3500, loss=0.45075110126704265\n",
      "Current iteration=3600, loss=0.45056529078014906\n",
      "Current iteration=3700, loss=0.4503892140420168\n",
      "Current iteration=3800, loss=0.4502220389112773\n",
      "Current iteration=3900, loss=0.4500630296766027\n",
      "Current iteration=4000, loss=0.4499115334867296\n",
      "Current iteration=4100, loss=0.4497669689508466\n",
      "Current iteration=4200, loss=0.4496288165276567\n",
      "Current iteration=4300, loss=0.44949661039413735\n",
      "Current iteration=4400, loss=0.44936993154284055\n",
      "Current iteration=4500, loss=0.4492484019027722\n",
      "Current iteration=4600, loss=0.4491316793159472\n",
      "Current iteration=4700, loss=0.44901945323158404\n",
      "Current iteration=4800, loss=0.4489114410040497\n",
      "Current iteration=4900, loss=0.44880738470028847\n",
      "Current iteration=5000, loss=0.44870704833844355\n",
      "Current iteration=5100, loss=0.44861021549246366\n",
      "Current iteration=5200, loss=0.4485166872082091\n",
      "Current iteration=5300, loss=0.448426280185408\n",
      "Current iteration=5400, loss=0.44833882518711\n",
      "Current iteration=5500, loss=0.4482541656443264\n",
      "Current iteration=5600, loss=0.4481721564285683\n",
      "Current iteration=5700, loss=0.4480926627691777\n",
      "Current iteration=5800, loss=0.44801555929583764\n",
      "Current iteration=5900, loss=0.44794072918957706\n",
      "Current iteration=6000, loss=0.44786806342804003\n",
      "Current iteration=6100, loss=0.4477974601128567\n",
      "Current iteration=6200, loss=0.4477288238686962\n",
      "Current iteration=6300, loss=0.44766206530505503\n",
      "Current iteration=6400, loss=0.44759710053308366\n",
      "Current iteration=6500, loss=0.4475338507308134\n",
      "Current iteration=6600, loss=0.4474722417510437\n",
      "Current iteration=6700, loss=0.4474122037669249\n",
      "Current iteration=6800, loss=0.4473536709509186\n",
      "Current iteration=6900, loss=0.44729658118338905\n",
      "Current iteration=7000, loss=0.4472408757875494\n",
      "Current iteration=7100, loss=0.4471864992879113\n",
      "Current iteration=7200, loss=0.4471333991897308\n",
      "Current iteration=7300, loss=0.4470815257772606\n",
      "Current iteration=7400, loss=0.4470308319288766\n",
      "Current iteration=7500, loss=0.44698127294737866\n",
      "Current iteration=7600, loss=0.4469328064039644\n",
      "Current iteration=7700, loss=0.4468853919945496\n",
      "Current iteration=7800, loss=0.44683899140725425\n",
      "Current iteration=7900, loss=0.4467935682000099\n",
      "Current iteration=8000, loss=0.44674908768735727\n",
      "Current iteration=8100, loss=0.44670551683560017\n",
      "Current iteration=8200, loss=0.44666282416557707\n",
      "Current iteration=8300, loss=0.44662097966238073\n",
      "Current iteration=8400, loss=0.44657995469143247\n",
      "Current iteration=8500, loss=0.446539721920372\n",
      "Current iteration=8600, loss=0.44650025524628145\n",
      "Current iteration=8700, loss=0.44646152972780645\n",
      "Current iteration=8800, loss=0.4464235215217794\n",
      "Current iteration=8900, loss=0.446386207823992\n",
      "Current iteration=9000, loss=0.44634956681378796\n",
      "Current iteration=9100, loss=0.44631357760218776\n",
      "Current iteration=9200, loss=0.44627822018327334\n",
      "Current iteration=9300, loss=0.44624347538859377\n",
      "Current iteration=9400, loss=0.4462093248443657\n",
      "Current iteration=9500, loss=0.4461757509312693\n",
      "Current iteration=9600, loss=0.4461427367466513\n",
      "Current iteration=9700, loss=0.44611026606896637\n",
      "Current iteration=9800, loss=0.44607832332430086\n",
      "Current iteration=9900, loss=0.4460468935548354\n",
      "Current iteration=10000, loss=0.446015962389115\n",
      "Current iteration=10100, loss=0.4459855160140037\n",
      "Current iteration=10200, loss=0.44595554114821473\n",
      "Current iteration=10300, loss=0.44592602501731116\n",
      "Current iteration=10400, loss=0.44589695533008167\n",
      "Current iteration=10500, loss=0.44586832025620493\n",
      "Current iteration=10600, loss=0.4458401084051186\n",
      "Current iteration=10700, loss=0.4458123088060206\n",
      "Current iteration=10800, loss=0.4457849108889287\n",
      "Current iteration=10900, loss=0.44575790446673746\n",
      "Current iteration=11000, loss=0.44573127971820975\n",
      "Current iteration=11100, loss=0.445705027171845\n",
      "Current iteration=11200, loss=0.4456791376905776\n",
      "Current iteration=11300, loss=0.4456536024572502\n",
      "Current iteration=11400, loss=0.4456284129608205\n",
      "Current iteration=11500, loss=0.44560356098325826\n",
      "Current iteration=11600, loss=0.4455790385870931\n",
      "Current iteration=11700, loss=0.44555483810357477\n",
      "Current iteration=11800, loss=0.445530952121414\n",
      "Current iteration=11900, loss=0.4455073734760695\n",
      "Current iteration=12000, loss=0.44548409523955035\n",
      "Current iteration=12100, loss=0.44546111071070954\n",
      "Current iteration=12200, loss=0.4454384134059953\n",
      "Current iteration=12300, loss=0.44541599705064233\n",
      "Current iteration=12400, loss=0.4453938555702746\n",
      "Current iteration=12500, loss=0.4453719830829003\n",
      "Current iteration=12600, loss=0.4453503738912767\n",
      "Current iteration=12700, loss=0.4453290224756269\n",
      "Current iteration=12800, loss=0.4453079234866893\n",
      "Current iteration=12900, loss=0.4452870717390833\n",
      "Current iteration=13000, loss=0.44526646220497373\n",
      "Current iteration=13100, loss=0.44524609000802057\n",
      "Current iteration=13200, loss=0.4452259504175977\n",
      "Current iteration=13300, loss=0.4452060388432693\n",
      "Current iteration=13400, loss=0.44518635082950875\n",
      "Current iteration=13500, loss=0.4451668820506492\n",
      "Current iteration=13600, loss=0.44514762830605487\n",
      "Current iteration=13700, loss=0.44512858551550066\n",
      "Current iteration=13800, loss=0.44510974971475264\n",
      "Current iteration=13900, loss=0.4450911170513355\n",
      "Current iteration=14000, loss=0.4450726837804823\n",
      "Current iteration=14100, loss=0.4450544462612557\n",
      "Current iteration=14200, loss=0.4450364009528318\n",
      "Current iteration=14300, loss=0.44501854441094046\n",
      "Current iteration=14400, loss=0.4450008732844546\n",
      "Current iteration=14500, loss=0.4449833843121199\n",
      "Current iteration=14600, loss=0.4449660743194201\n",
      "Current iteration=14700, loss=0.44494894021557113\n",
      "Current iteration=14800, loss=0.4449319789906371\n",
      "Current iteration=14900, loss=0.444915187712765\n",
      "Current iteration=0, loss=1.4279485277215551\n",
      "Current iteration=100, loss=0.6059784006993191\n",
      "Current iteration=200, loss=0.5386634539183024\n",
      "Current iteration=300, loss=0.5061982049255203\n",
      "Current iteration=400, loss=0.4855723845401673\n",
      "Current iteration=500, loss=0.4709192230108702\n",
      "Current iteration=600, loss=0.4598590084212009\n",
      "Current iteration=700, loss=0.45117464212127517\n",
      "Current iteration=800, loss=0.4441667685258087\n",
      "Current iteration=900, loss=0.4383938294065196\n",
      "Current iteration=1000, loss=0.43355981740277777\n",
      "Current iteration=1100, loss=0.42945732710856355\n",
      "Current iteration=1200, loss=0.42593577189735427\n",
      "Current iteration=1300, loss=0.4228829555719691\n",
      "Current iteration=1400, loss=0.4202136628155595\n",
      "Current iteration=1500, loss=0.4178621085216844\n",
      "Current iteration=1600, loss=0.4157767425880563\n",
      "Current iteration=1700, loss=0.4139165981294642\n",
      "Current iteration=1800, loss=0.41224868280410326\n",
      "Current iteration=1900, loss=0.4107460912457377\n",
      "Current iteration=2000, loss=0.40938662978125423\n",
      "Current iteration=2100, loss=0.408151810872055\n",
      "Current iteration=2200, loss=0.40702611087410306\n",
      "Current iteration=2300, loss=0.4059964101907445\n",
      "Current iteration=2400, loss=0.40505155832904843\n",
      "Current iteration=2500, loss=0.40418202628580857\n",
      "Current iteration=2600, loss=0.40337962335326716\n",
      "Current iteration=2700, loss=0.40263726545669626\n",
      "Current iteration=2800, loss=0.4019487879727748\n",
      "Current iteration=2900, loss=0.40130879753615384\n",
      "Current iteration=3000, loss=0.40071255597992805\n",
      "Current iteration=3100, loss=0.40015588828320475\n",
      "Current iteration=3200, loss=0.3996351071920205\n",
      "Current iteration=3300, loss=0.3991469494694851\n",
      "Current iteration=3400, loss=0.39868852094198715\n",
      "Current iteration=3500, loss=0.3982572488427132\n",
      "Current iteration=3600, loss=0.3978508405224619\n",
      "Current iteration=3700, loss=0.3974672477787564\n",
      "Current iteration=3800, loss=0.39710463611056457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=3900, loss=0.3967613582477023\n",
      "Current iteration=4000, loss=0.39643593135831745\n",
      "Current iteration=4100, loss=0.3961270174027704\n",
      "Current iteration=4200, loss=0.39583340617028484\n",
      "Current iteration=4300, loss=0.39555400060010565\n",
      "Current iteration=4400, loss=0.39528780404831976\n",
      "Current iteration=4500, loss=0.3950339092136641\n",
      "Current iteration=4600, loss=0.3947914884804651\n",
      "Current iteration=4700, loss=0.394559785474835\n",
      "Current iteration=4800, loss=0.3943381076621795\n",
      "Current iteration=4900, loss=0.3941258198407918\n",
      "Current iteration=5000, loss=0.39392233840863355\n",
      "Current iteration=5100, loss=0.3937271262990464\n",
      "Current iteration=5200, loss=0.39353968849672677\n",
      "Current iteration=5300, loss=0.393359568058357\n",
      "Current iteration=5400, loss=0.3931863425732409\n",
      "Current iteration=5500, loss=0.3930196210085223\n",
      "Current iteration=5600, loss=0.3928590408913437\n",
      "Current iteration=5700, loss=0.39270426578689177\n",
      "Current iteration=5800, loss=0.3925549830368618\n",
      "Current iteration=5900, loss=0.39241090172762666\n",
      "Current iteration=6000, loss=0.39227175086145033\n",
      "Current iteration=6100, loss=0.3921372777075497\n",
      "Current iteration=6200, loss=0.39200724631278056\n",
      "Current iteration=6300, loss=0.3918814361542754\n",
      "Current iteration=6400, loss=0.39175964091855386\n",
      "Current iteration=6500, loss=0.39164166739352757\n",
      "Current iteration=6600, loss=0.39152733446145593\n",
      "Current iteration=6700, loss=0.391416472182333\n",
      "Current iteration=6800, loss=0.3913089209584168\n",
      "Current iteration=6900, loss=0.391204530771687\n",
      "Current iteration=7000, loss=0.3911031604869542\n",
      "Current iteration=7100, loss=0.3910046772141589\n",
      "Current iteration=7200, loss=0.3909089557241134\n",
      "Current iteration=7300, loss=0.3908158779125743\n",
      "Current iteration=7400, loss=0.39072533230807527\n",
      "Current iteration=7500, loss=0.39063721361944065\n",
      "Current iteration=7600, loss=0.39055142231932966\n",
      "Current iteration=7700, loss=0.39046786426053426\n",
      "Current iteration=7800, loss=0.39038645032209296\n",
      "Current iteration=7900, loss=0.3903070960825772\n",
      "Current iteration=8000, loss=0.39022972151816965\n",
      "Current iteration=8100, loss=0.3901542507233905\n",
      "Current iteration=8200, loss=0.3900806116525349\n",
      "Current iteration=8300, loss=0.39000873588006785\n",
      "Current iteration=8400, loss=0.3899385583783965\n",
      "Current iteration=8500, loss=0.38987001731158044\n",
      "Current iteration=8600, loss=0.3898030538436776\n",
      "Current iteration=8700, loss=0.38973761196054485\n",
      "Current iteration=8800, loss=0.3896736383040135\n",
      "Current iteration=8900, loss=0.38961108201746414\n",
      "Current iteration=9000, loss=0.3895498946019056\n",
      "Current iteration=9100, loss=0.38949002978174435\n",
      "Current iteration=9200, loss=0.3894314433795019\n",
      "Current iteration=9300, loss=0.3893740931988008\n",
      "Current iteration=9400, loss=0.3893179389149942\n",
      "Current iteration=9500, loss=0.38926294197287487\n",
      "Current iteration=9600, loss=0.38920906549093776\n",
      "Current iteration=9700, loss=0.38915627417171994\n",
      "Current iteration=9800, loss=0.3891045342177757\n",
      "Current iteration=9900, loss=0.3890538132528859\n",
      "Current iteration=10000, loss=0.389004080248128\n",
      "Current iteration=10100, loss=0.3889553054524655\n",
      "Current iteration=10200, loss=0.38890746032754103\n",
      "Current iteration=10300, loss=0.3888605174863846\n",
      "Current iteration=10400, loss=0.3888144506357661\n",
      "Current iteration=10500, loss=0.38876923452194734\n",
      "Current iteration=10600, loss=0.388724844879603\n",
      "Current iteration=10700, loss=0.38868125838370104\n",
      "Current iteration=10800, loss=0.3886384526041443\n",
      "Current iteration=10900, loss=0.3885964059629949\n",
      "Current iteration=11000, loss=0.38855509769411206\n",
      "Current iteration=11100, loss=0.38851450780504787\n",
      "Current iteration=11200, loss=0.38847461704105646\n",
      "Current iteration=11300, loss=0.38843540685108385\n",
      "Current iteration=11400, loss=0.3883968593556118\n",
      "Current iteration=11500, loss=0.38835895731624215\n",
      "Current iteration=11600, loss=0.38832168410691187\n",
      "Current iteration=11700, loss=0.38828502368664014\n",
      "Current iteration=11800, loss=0.38824896057371394\n",
      "Current iteration=11900, loss=0.3882134798212242\n",
      "Current iteration=12000, loss=0.38817856699387265\n",
      "Current iteration=12100, loss=0.38814420814597406\n",
      "Current iteration=12200, loss=0.38811038980057977\n",
      "Current iteration=12300, loss=0.3880770989296638\n",
      "Current iteration=12400, loss=0.3880443229353013\n",
      "Current iteration=12500, loss=0.3880120496317886\n",
      "Current iteration=12600, loss=0.3879802672286485\n",
      "Current iteration=12700, loss=0.3879489643144685\n",
      "Current iteration=12800, loss=0.38791812984152957\n",
      "Current iteration=12900, loss=0.3878877531111765\n",
      "Current iteration=13000, loss=0.38785782375989086\n",
      "Current iteration=13100, loss=0.38782833174602854\n",
      "Current iteration=13200, loss=0.3877992673371832\n",
      "Current iteration=13300, loss=0.3877706210981424\n",
      "Current iteration=13400, loss=0.3877423838794067\n",
      "Current iteration=13500, loss=0.38771454680623635\n",
      "Current iteration=13600, loss=0.38768710126820366\n",
      "Current iteration=13700, loss=0.3876600389092189\n",
      "Current iteration=13800, loss=0.3876333516180084\n",
      "Current iteration=13900, loss=0.3876070315190192\n",
      "Current iteration=14000, loss=0.38758107096372996\n",
      "Current iteration=14100, loss=0.38755546252234485\n",
      "Current iteration=14200, loss=0.38753019897585333\n",
      "Current iteration=14300, loss=0.38750527330843515\n",
      "Current iteration=14400, loss=0.38748067870019537\n",
      "Current iteration=14500, loss=0.38745640852021007\n",
      "Current iteration=14600, loss=0.38743245631986983\n",
      "Current iteration=14700, loss=0.3874088158265052\n",
      "Current iteration=14800, loss=0.38738548093727976\n",
      "Current iteration=14900, loss=0.3873624457133396\n",
      "Current iteration=0, loss=1.155021892594513\n",
      "Current iteration=100, loss=0.6667604655775046\n",
      "Current iteration=200, loss=0.5963084453354935\n",
      "Current iteration=300, loss=0.5566371194342148\n",
      "Current iteration=400, loss=0.5304152493378537\n",
      "Current iteration=500, loss=0.5116524340234494\n",
      "Current iteration=600, loss=0.49755132455120565\n",
      "Current iteration=700, loss=0.4866076904478381\n",
      "Current iteration=800, loss=0.4779167863301092\n",
      "Current iteration=900, loss=0.4708944149627956\n",
      "Current iteration=1000, loss=0.46512471743635775\n",
      "Current iteration=1100, loss=0.4603083523309933\n",
      "Current iteration=1200, loss=0.4562326993805464\n",
      "Current iteration=1300, loss=0.45274282540110566\n",
      "Current iteration=1400, loss=0.44972328361798536\n",
      "Current iteration=1500, loss=0.44708649364831254\n",
      "Current iteration=1600, loss=0.4447649628765078\n",
      "Current iteration=1700, loss=0.44270593312016343\n",
      "Current iteration=1800, loss=0.4408676219085465\n",
      "Current iteration=1900, loss=0.43921653999769655\n",
      "Current iteration=2000, loss=0.43772554892583093\n",
      "Current iteration=2100, loss=0.4363724348713892\n",
      "Current iteration=2200, loss=0.43513884762058375\n",
      "Current iteration=2300, loss=0.4340095013020698\n",
      "Current iteration=2400, loss=0.43297156527977854\n",
      "Current iteration=2500, loss=0.4320141947945589\n",
      "Current iteration=2600, loss=0.43112816538256377\n",
      "Current iteration=2700, loss=0.43030558515707196\n",
      "Current iteration=2800, loss=0.4295396661479354\n",
      "Current iteration=2900, loss=0.42882454093567507\n",
      "Current iteration=3000, loss=0.42815511440032294\n",
      "Current iteration=3100, loss=0.4275269429643702\n",
      "Current iteration=3200, loss=0.4269361355566874\n",
      "Current iteration=3300, loss=0.42637927187652724\n",
      "Current iteration=3400, loss=0.4258533345404134\n",
      "Current iteration=3500, loss=0.42535565244868057\n",
      "Current iteration=3600, loss=0.4248838532801767\n",
      "Current iteration=3700, loss=0.42443582346041214\n",
      "Current iteration=3800, loss=0.4240096742842528\n",
      "Current iteration=3900, loss=0.4236037131341252\n",
      "Current iteration=4000, loss=0.42321641893727274\n",
      "Current iteration=4100, loss=0.42284642116480314\n",
      "Current iteration=4200, loss=0.4224924818014705\n",
      "Current iteration=4300, loss=0.422153479816071\n",
      "Current iteration=4400, loss=0.4218283977436968\n",
      "Current iteration=4500, loss=0.42151631005714746\n",
      "Current iteration=4600, loss=0.42121637305874393\n",
      "Current iteration=4700, loss=0.4209278160680332\n",
      "Current iteration=4800, loss=0.42064993371730863\n",
      "Current iteration=4900, loss=0.4203820791969593\n",
      "Current iteration=5000, loss=0.4201236583175739\n",
      "Current iteration=5100, loss=0.41987412427640036\n",
      "Current iteration=5200, loss=0.4196329730329636\n",
      "Current iteration=5300, loss=0.41939973921298146\n",
      "Current iteration=5400, loss=0.4191739924717074\n",
      "Current iteration=5500, loss=0.41895533425786896\n",
      "Current iteration=5600, loss=0.4187433949278075\n",
      "Current iteration=5700, loss=0.4185378311665329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=5800, loss=0.41833832367840607\n",
      "Current iteration=5900, loss=0.4181445751152472\n",
      "Current iteration=6000, loss=0.41795630821398705\n",
      "Current iteration=6100, loss=0.41777326411965177\n",
      "Current iteration=6200, loss=0.41759520087261565\n",
      "Current iteration=6300, loss=0.41742189204174346\n",
      "Current iteration=6400, loss=0.4172531254873546\n",
      "Current iteration=6500, loss=0.4170887022399245\n",
      "Current iteration=6600, loss=0.4169284354821603\n",
      "Current iteration=6700, loss=0.41677214962356374\n",
      "Current iteration=6800, loss=0.4166196794578858\n",
      "Current iteration=6900, loss=0.41647086939499317\n",
      "Current iteration=7000, loss=0.41632557275964016\n",
      "Current iteration=7100, loss=0.41618365115048855\n",
      "Current iteration=7200, loss=0.4160449738534642\n",
      "Current iteration=7300, loss=0.4159094173041844\n",
      "Current iteration=7400, loss=0.41577686459476965\n",
      "Current iteration=7500, loss=0.41564720502084845\n",
      "Current iteration=7600, loss=0.41552033366501295\n",
      "Current iteration=7700, loss=0.4153961510133714\n",
      "Current iteration=7800, loss=0.41527456260219064\n",
      "Current iteration=7900, loss=0.4151554786919286\n",
      "Current iteration=8000, loss=0.41503881396622705\n",
      "Current iteration=8100, loss=0.4149244872536783\n",
      "Current iteration=8200, loss=0.4148124212703944\n",
      "Current iteration=8300, loss=0.41470254238159704\n",
      "Current iteration=8400, loss=0.4145947803806206\n",
      "Current iteration=8500, loss=0.4144890682838701\n",
      "Current iteration=8600, loss=0.41438534214041595\n",
      "Current iteration=8700, loss=0.41428354085502683\n",
      "Current iteration=8800, loss=0.4141836060235543\n",
      "Current iteration=8900, loss=0.4140854817796813\n",
      "Current iteration=9000, loss=0.41398911465213284\n",
      "Current iteration=9100, loss=0.413894453431532\n",
      "Current iteration=9200, loss=0.41380144904615085\n",
      "Current iteration=9300, loss=0.4137100544458767\n",
      "Current iteration=9400, loss=0.41362022449376723\n",
      "Current iteration=9500, loss=0.4135319158646267\n",
      "Current iteration=9600, loss=0.41344508695007864\n",
      "Current iteration=9700, loss=0.4133596977696579\n",
      "Current iteration=9800, loss=0.4132757098874821\n",
      "Current iteration=9900, loss=0.41319308633410057\n",
      "Current iteration=10000, loss=0.41311179153314803\n",
      "Current iteration=10100, loss=0.41303179123246536\n",
      "Current iteration=10200, loss=0.41295305243937136\n",
      "Current iteration=10300, loss=0.41287554335979826\n",
      "Current iteration=10400, loss=0.4127992333410238\n",
      "Current iteration=10500, loss=0.41272409281775413\n",
      "Current iteration=10600, loss=0.412650093261332\n",
      "Current iteration=10700, loss=0.4125772071318579\n",
      "Current iteration=10800, loss=0.41250540783303347\n",
      "Current iteration=10900, loss=0.4124346696695447\n",
      "Current iteration=11000, loss=0.4123649678068212\n",
      "Current iteration=11100, loss=0.4122962782330162\n",
      "Current iteration=11200, loss=0.4122285777230642\n",
      "Current iteration=11300, loss=0.4121618438046852\n",
      "Current iteration=11400, loss=0.4120960547262109\n",
      "Current iteration=11500, loss=0.41203118942611977\n",
      "Current iteration=11600, loss=0.4119672275041738\n",
      "Current iteration=11700, loss=0.41190414919405854\n",
      "Current iteration=11800, loss=0.4118419353374339\n",
      "Current iteration=11900, loss=0.41178056735931184\n",
      "Current iteration=12000, loss=0.4117200272446778\n",
      "Current iteration=12100, loss=0.4116602975162847\n",
      "Current iteration=12200, loss=0.4116013612135482\n",
      "Current iteration=12300, loss=0.4115432018724796\n",
      "Current iteration=12400, loss=0.41148580350659414\n",
      "Current iteration=12500, loss=0.41142915058874063\n",
      "Current iteration=12600, loss=0.41137322803379767\n",
      "Current iteration=12700, loss=0.4113180211821884\n",
      "Current iteration=12800, loss=0.4112635157841668\n",
      "Current iteration=12900, loss=0.411209697984833\n",
      "Current iteration=13000, loss=0.41115655430983683\n",
      "Current iteration=13100, loss=0.41110407165173346\n",
      "Current iteration=13200, loss=0.41105223725695317\n",
      "Current iteration=13300, loss=0.4110010387133551\n",
      "Current iteration=13400, loss=0.41095046393833307\n",
      "Current iteration=13500, loss=0.4109005011674453\n",
      "Current iteration=13600, loss=0.4108511389435416\n",
      "Current iteration=13700, loss=0.4108023661063642\n",
      "Current iteration=13800, loss=0.4107541717825997\n",
      "Current iteration=13900, loss=0.4107065453763618\n",
      "Current iteration=14000, loss=0.41065947656008855\n",
      "Current iteration=14100, loss=0.4106129552658368\n",
      "Current iteration=14200, loss=0.4105669716769641\n",
      "Current iteration=14300, loss=0.4105215162201864\n",
      "Current iteration=14400, loss=0.41047657955800765\n",
      "Current iteration=14500, loss=0.4104321525815206\n",
      "Current iteration=14600, loss=0.41038822640358164\n",
      "Current iteration=14700, loss=0.410344792352373\n",
      "Current iteration=14800, loss=0.4103018419653711\n",
      "Current iteration=14900, loss=0.41025936698375026\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w_0 = np.random.normal(0., 0.1, [tx_train_0.shape[1],])\n",
    "initial_w_1 = np.random.normal(0., 0.1, [tx_train_1.shape[1],])\n",
    "initial_w_2 = np.random.normal(0., 0.1, [tx_train_2.shape[1],])\n",
    "initial_w_3 = np.random.normal(0., 0.1, [tx_train_3.shape[1],])\n",
    "\n",
    "# Train models\n",
    "w_0, train_loss_0 = impl.reg_logistic_regression(y_0, tx_train_0, 0.001, initial_w_0, max_iters=15000, gamma=0.1)\n",
    "w_1, train_loss_1 = impl.reg_logistic_regression(y_1, tx_train_1, 0.001, initial_w_1, max_iters=15000, gamma=0.01)\n",
    "w_2, train_loss_2 = impl.reg_logistic_regression(y_2, tx_train_2, 0.001, initial_w_2, max_iters=15000, gamma=0.01)\n",
    "w_3, train_loss_3 = impl.reg_logistic_regression(y_3, tx_train_3, 0.001, initial_w_3, max_iters=15000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab257b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_0 = 0.35681458766582386\n",
      "train_loss_1 = 0.4448985635255296\n",
      "train_loss_2 = 0.38733970437420373\n",
      "train_loss_3 = 0.41021735934726467\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_loss_0 = {train_loss_0}\")\n",
    "print(f\"train_loss_1 = {train_loss_1}\")\n",
    "print(f\"train_loss_2 = {train_loss_2}\")\n",
    "print(f\"train_loss_3 = {train_loss_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf50be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy_0 = 0.8421226467026313\n",
      "train_accuracy_1 = 0.800448777468276\n",
      "train_accuracy_2 = 0.8378888028742134\n",
      "train_accuracy_3 = 0.8253474102147627\n",
      "train_accuracy = 0.826856\n"
     ]
    }
   ],
   "source": [
    "# Compute training accuracies\n",
    "predict_train_0 = helpers.predict_logistic(tx_train_0, w_0)\n",
    "predict_train_1 = helpers.predict_logistic(tx_train_1, w_1)\n",
    "predict_train_2 = helpers.predict_logistic(tx_train_2, w_2)\n",
    "predict_train_3 = helpers.predict_logistic(tx_train_3, w_3)\n",
    "\n",
    "predict_train_0[predict_train_0 == -1] = 0\n",
    "predict_train_1[predict_train_1 == -1] = 0\n",
    "predict_train_2[predict_train_2 == -1] = 0\n",
    "predict_train_3[predict_train_3 == -1] = 0\n",
    "\n",
    "predict_train = np.concatenate((predict_train_0, predict_train_1, predict_train_2, predict_train_3))\n",
    "\n",
    "train_accuracy_0 = helpers.accuracy(predict_train_0, y_0)\n",
    "train_accuracy_1 = helpers.accuracy(predict_train_1, y_1)\n",
    "train_accuracy_2 = helpers.accuracy(predict_train_2, y_2)\n",
    "train_accuracy_3 = helpers.accuracy(predict_train_3, y_3)\n",
    "train_accuracy = helpers.accuracy(predict_train, np.concatenate((y_0, y_1, y_2, y_3)))\n",
    "\n",
    "print(f\"train_accuracy_0 = {train_accuracy_0}\")\n",
    "print(f\"train_accuracy_1 = {train_accuracy_1}\")\n",
    "print(f\"train_accuracy_2 = {train_accuracy_2}\")\n",
    "print(f\"train_accuracy_3 = {train_accuracy_3}\")\n",
    "print(f\"train_accuracy = {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5193e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d8c7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean data into 4 sets according to 22nd feature\n",
    "tx_test_0, y_test_0, ids_test_0, _ = helpers.split_i(tx_test, y_test, ids_test, 0, miss_col_0)\n",
    "tx_test_1, y_test_1, ids_test_1, _ = helpers.split_i(tx_test, y_test, ids_test, 1, miss_col_1)\n",
    "tx_test_2, y_test_2, ids_test_2, _ = helpers.split_i(tx_test, y_test, ids_test, 2, miss_col_2)\n",
    "tx_test_3, y_test_3, ids_test_3, _ = helpers.split_i(tx_test, y_test, ids_test, 3, miss_col_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6ea7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "tx_test_0, _, _ = helpers.standardize(tx_test_0, mean_0, std_0)\n",
    "tx_test_1, _, _ = helpers.standardize(tx_test_1, mean_1, std_1)\n",
    "tx_test_2, _, _ = helpers.standardize(tx_test_2, mean_2, std_2)\n",
    "tx_test_3, _, _ = helpers.standardize(tx_test_3, mean_3, std_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "836bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to degree 2\n",
    "tx_test_0 = helpers.build_poly_deg2(tx_test_0)\n",
    "tx_test_1 = helpers.build_poly_deg2(tx_test_1)\n",
    "tx_test_2 = helpers.build_poly_deg2(tx_test_2)\n",
    "tx_test_3 = helpers.build_poly_deg2(tx_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "589f2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_test_0 = np.c_[np.ones((tx_test_0.shape[0], 1)), tx_test_0]\n",
    "tx_test_1 = np.c_[np.ones((tx_test_1.shape[0], 1)), tx_test_1]\n",
    "tx_test_2 = np.c_[np.ones((tx_test_2.shape[0], 1)), tx_test_2]\n",
    "tx_test_3 = np.c_[np.ones((tx_test_3.shape[0], 1)), tx_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba8668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels\n",
    "predict_test_0 = helpers.predict_logistic(tx_test_0, w_0)\n",
    "predict_test_1 = helpers.predict_logistic(tx_test_1, w_1)\n",
    "predict_test_2 = helpers.predict_logistic(tx_test_2, w_2)\n",
    "predict_test_3 = helpers.predict_logistic(tx_test_3, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04bd7443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1.  1. ... -1.  1. -1.]\n",
      "[-1. -1. -1. ...  1.  1. -1.]\n",
      "[-1.  1. -1. ... -1.  1.  1.]\n",
      "[-1. -1. -1. ...  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(predict_test_0)\n",
    "print(predict_test_1)\n",
    "print(predict_test_2)\n",
    "print(predict_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e6f643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets\n",
    "predict_test = np.concatenate((predict_test_0, predict_test_1, predict_test_2, predict_test_3))\n",
    "ids_test = np.concatenate((ids_test_0, ids_test_1, ids_test_2, ids_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5f84a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238,)\n"
     ]
    }
   ],
   "source": [
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c95d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Reg_Logistic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ba3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
