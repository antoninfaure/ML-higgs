{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b8334db5f84f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(path_dataset,sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", dtype=str,  skip_header=1)\n",
    "    ids = data[:,0]\n",
    "    labels = data[:,1]\n",
    "    labels[labels=='s']=1\n",
    "    labels[labels=='b']=-1\n",
    "    #labels[labels=='?']=0\n",
    "    labels = np.asarray(labels, dtype=float)\n",
    "    data = np.delete(data, [0,1], 1)\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(path_dataset,sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", dtype=str,  skip_header=1)\n",
    "    ids = data[:,0]\n",
    "    labels = data[:,1]\n",
    "    data = np.delete(data, [0,1], 1)\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of :\n",
    "\n",
    "- type s is assigned value 0\n",
    "- type b is assigned value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, train_labels, ids = load_data_train('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    # Remove columns with more than 50% of -999\n",
    "    dirty_cols = np.where(np.sum(train_data == -999, axis=0)/train_data.shape[0] < 0.5, True, False)\n",
    "    data = data[:, dirty_cols]\n",
    "    # Replace -999 by nan\n",
    "    data = np.where(data == -999, np.nan, data)\n",
    "    # Compute the columns means without nan values \n",
    "    means = np.nanmean(data, axis=0)\n",
    "    #Find indices that you need to replace\n",
    "    inds = np.where(np.isnan(data))\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    data[inds] = np.take(means, inds[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_old(data):\n",
    "    # remove the columns containing a -999 value\n",
    "    valid_cols = np.all(data!=-999, axis=0)\n",
    "    return data[:,valid_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "def standardize(x):\n",
    "    #Standardize the original data set.\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = standardize(clean_data(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_reg(labels, data, w, lambda_):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = np.sum(np.logaddexp(0, data @ w) + labels * data.dot(w)) + lambda_*np.linalg.norm(w)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_old(labels, data, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(labels, data, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = np.sum(np.logaddexp(0, data @ w) + labels * data.dot(w))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(tx):\n",
    "    initial_w = np.random.normal(0., 0.1, [tx.shape[1],])\n",
    "    return initial_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Labels of shape (N, ).\n",
    "        tx (np.array): Dataset of shape (N, D).\n",
    "        initial_w (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (integer): Step size\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"  \n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    \n",
    "    w = initialize_weights(tx)\n",
    "    \n",
    "    for it in range(max_iters):\n",
    "        #loss = np.sum(np.logaddexp(0, tx.dot(w)) + y * tx.dot(w))\n",
    "        grad = tx.T.dot(sigmoid(tx.dot(w)) - y)\n",
    "        w -= gamma * grad\n",
    "        # log info\n",
    "        if it % 100 == 0:\n",
    "            print(f\"Current iteration={it}\")\n",
    "        #    print(\"Current iteration={i}, loss={l}\".format(i=it, l=loss))\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "    loss = calculate_loss_reg(y, tx, w)\n",
    "    print(\"loss={l}\".format(l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, max_iters, gamma):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Labels of shape (N, ).\n",
    "        tx (np.array): Dataset of shape (N, D).\n",
    "        lambda_ (integer): Regularization factor\n",
    "        initial_w (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (integer): Step size\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"  \n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    \n",
    "    w = initialize_weights(tx)\n",
    "    \n",
    "    for it in range(max_iters):\n",
    "        #loss = -np.sum(y * np.log(sigmoid(tx @ w)) + (1 - y) * np.log(1 - sigmoid(tx @ w))) + lambda_*np.linalg.norm(w)**2\n",
    "        grad = tx.T.dot(sigmoid(tx.dot(w)) - y) + 2*lambda_*w\n",
    "        w -= gamma * grad\n",
    "        # log info\n",
    "        #if it % 100 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=it, l=loss))\n",
    "            #print(f\"Current iteration={it}\")\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        #    break\n",
    "    loss = calculate_loss_reg(y, tx, w, lambda_)\n",
    "    print(\"loss={l}\".format(l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_ls(x, y, lambda_):\n",
    "    \"\"\"ridge regression for least squares loss function\"\"\"\n",
    "    A = x.T.dot(x) + 2*len(x)*lambda_*np.identity(x.shape[1])\n",
    "    B = x.T.dot(y)\n",
    "    return np.linalg.solve(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"split the dataset based on the split ratio.\"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # generate random indices\n",
    "    num_row = len(y)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    index_split = int(np.floor(ratio * num_row))\n",
    "    index_tr = indices[: index_split]\n",
    "    index_te = indices[index_split:]\n",
    "    # create split\n",
    "    x_tr = x[index_tr]\n",
    "    x_te = x[index_te]\n",
    "    y_tr = y[index_tr]\n",
    "    y_te = y[index_te]\n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_old(x, y, ratio, lambda_, max_iters, gamma):\n",
    "    num_row = len(y)\n",
    "    index_split = int(np.floor(ratio*num_row))\n",
    "    k = len(y)-index_split-1\n",
    "    ws = []\n",
    "    losses = []\n",
    "    for i in range(0,k):\n",
    "        index_te = indices[i:i+index_split]\n",
    "        x_te = x[index_te]\n",
    "        y_te = y[index_te]\n",
    "        x_tr = np.delete(x, index_te, 0)\n",
    "        y_tr = np.delete(y, index_te, 0)\n",
    "        w, loss_tr = reg_logistic_regression(y_tr, x_tr, lambda_, max_iters, gamma)\n",
    "        loss_te = calculate_loss(y_te, x_te)\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "    return ws, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, max_iters, gamma):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_index = k_indices[k]\n",
    "    tr_index = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_index = tr_index.reshape(-1)\n",
    "    y_te = y[te_index]\n",
    "    y_tr = y[tr_index]\n",
    "    x_te = x[te_index]\n",
    "    x_tr = x[tr_index]\n",
    "    # weights and training loss for logistic regression model:\n",
    "    w, loss_tr = reg_logistic_regression(y_tr, x_tr, lambda_, max_iters, gamma) \n",
    "    # calculate the loss for test data:\n",
    "    loss_te = calculate_loss_reg(y_te, x_te, w, lambda_)\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_lambda_selection(y, x, k_fold, max_iters, gamma):\n",
    "    seed = 12\n",
    "    lambdas = np.logspace(-4, 0, 3) #change last parameter to 10 if it works\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    loss_training = []\n",
    "    loss_test = []\n",
    "    # cross validation\n",
    "    for lambda_ in lambdas:\n",
    "        print(f\"Current lambda={lambda_}\")\n",
    "        loss_tr_tmp = []\n",
    "        loss_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            print(f\"Current iteration={k}\")\n",
    "            loss_tr, loss_te,_ = cross_validation(y, x, k_indices, k, lambda_, max_iters, gamma)\n",
    "            loss_tr_tmp.append(loss_tr)\n",
    "            loss_te_tmp.append(loss_te)\n",
    "        loss_training.append(np.mean(loss_tr_tmp))\n",
    "        loss_test.append(np.mean(loss_te_tmp))\n",
    "        \n",
    "    ind_best_lambda = np.argmin(loss_test)\n",
    "    best_lambda = lambdas[ind_best_lambda]\n",
    "    \n",
    "    return best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic(tx, w):\n",
    "    def sigmoid(t):\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "    y = sigmoid(tx @ w)\n",
    "    # s = 1 , b = -1\n",
    "    y[y < 0.5] = 1\n",
    "    y[y >= 0.5] = -1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(a, b):\n",
    "    return np.sum(a == b)/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_submission(ids, y_pred, name):\n",
    "\n",
    "    with open (name, 'w') as csvfile:\n",
    "        fd = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fd)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip (ids, y_pred):\n",
    "            writer.writerow({'Id' : int(r1), 'Prediction': str(r2)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing on test data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx = np.c_[np.ones((train_labels.shape[0], 1)), new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current lambda=0.0001\n",
      "Current iteration=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-0cad211ba73a>:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=5340012927409.967\n",
      "Current iteration=1\n",
      "loss=5330004467709.197\n",
      "Current iteration=2\n",
      "loss=5384708190546.278\n",
      "Current iteration=3\n",
      "loss=5375161714547.664\n",
      "Current lambda=0.01\n",
      "Current iteration=0\n",
      "loss=779417096984.3026\n",
      "Current iteration=1\n",
      "loss=778050813501.0726\n",
      "Current iteration=2\n",
      "loss=785713013933.1128\n",
      "Current iteration=3\n",
      "loss=784524777305.6493\n",
      "Current lambda=1.0\n",
      "Current iteration=0\n",
      "loss=7794320816.6141205\n",
      "Current iteration=1\n",
      "loss=7780995663.842256\n",
      "Current iteration=2\n",
      "loss=7857124945.090847\n",
      "Current iteration=3\n",
      "loss=7845630214.326981\n",
      "Optimal lambda:  1.0\n",
      "loss=13901029251.560066\n"
     ]
    }
   ],
   "source": [
    "#find optimal value for lambda (such that loss is minimized):\n",
    "lambda_ = best_lambda_selection(train_labels, tx, 4, 5000, 0.1)\n",
    "print(\"Optimal lambda: \",lambda_)\n",
    "#Compute model weights and loss with optimal lambda:\n",
    "trained_weights, train_loss = reg_logistic_regression(train_labels, tx, lambda_, max_iters=5000, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and standardize test data:\n",
    "test_data, labels, ids = load_data('test.csv')\n",
    "test_data = standardize(clean_data(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-ecbc95861a45>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "#Generate labels for test data:\n",
    "tx_test = np.c_[np.ones((test_data.shape[0], 1)), test_data]\n",
    "predicted_labels = predict_logistic(tx_test, trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_submission(ids, predicted_labels, 'Cross_Validation_Prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
