{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl\n",
    "import cross_validation as cv\n",
    "import hyperparameter_opti as hpopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, train_labels, ids = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize data\n",
    "new_data = helpers.standardize(helpers.clean_data(train_data))\n",
    "train_labels[train_labels==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add bias to data:\n",
    "new_data = np.c_[np.ones((new_data.shape[0], 1)), new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data:\n",
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "#Standardize test data:\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))\n",
    "#Add bias to test data:\n",
    "tx_test = np.c_[np.ones((tx_test.shape[0], 1)), tx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least Squares:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ls, loss_ls = impl.least_squares(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = impl.test_data(weights_ls, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_LS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best degree to build a polynomial basis from the training data:\n",
    "degree_opt, lambda_opt = cv.best_degree_selection(train_labels, new_data, np.arange(2,5), 4, np.logspace(-4,0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(degree_opt)\n",
    "print(lambda_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(new_data.shape)\n",
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 63)\n",
      "(63,)\n"
     ]
    }
   ],
   "source": [
    "#build polynomial basis from the entire training set using the optimal degree:\n",
    "xpoly = cv.build_poly(new_data, degree_opt)\n",
    "print(xpoly.shape)\n",
    "#compute weights and loss for the optimal lambda:\n",
    "w_ridge, loss_ridge = impl.ridge_regression(train_labels, xpoly, lambda_opt)\n",
    "print(w_ridge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 63)\n"
     ]
    }
   ],
   "source": [
    "xpoly_test = cv.build_poly(tx_test, degree_opt)\n",
    "print(xpoly_test.shape)\n",
    "y_test = impl.test_data(w_ridge, xpoly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_RG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice into training and validation sets\n",
    "y_validation, y_train, tx_validation, tx_train = helpers.slice_data(train_labels, new_data, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), tx_train]\n",
    "tx_validation = np.c_[np.ones((y_validation.shape[0], 1)), tx_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [tx_train.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.7163567950814637\n",
      "Current iteration=100, loss=0.6227581517924388\n",
      "Current iteration=200, loss=0.584904560206431\n",
      "Current iteration=300, loss=0.5641307100362332\n",
      "Current iteration=400, loss=0.5512503018068265\n",
      "Current iteration=500, loss=0.5426562963788711\n",
      "Current iteration=600, loss=0.5366105979203447\n",
      "Current iteration=700, loss=0.5321775687371848\n",
      "Current iteration=800, loss=0.5288137652729488\n",
      "Current iteration=900, loss=0.526185117200647\n"
     ]
    }
   ],
   "source": [
    "#Find the value of gamma that minimizes the loss:\n",
    "gamma_opt = hpopt.best_gamma_selection(y_train, tx_train, 1000)\n",
    "# Train model\n",
    "trained_weights, train_loss = impl.logistic_regression(y_train, tx_train, initial_w, max_iters=1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "predict_validation = helpers.predict_logistic(tx_validation, trained_weights)\n",
    "predict_train = helpers.predict_logistic(tx_train, trained_weights)\n",
    "\n",
    "predict_validation[predict_validation == -1] = 0\n",
    "predict_train[predict_train == -1] = 0\n",
    "\n",
    "train_accuracy = helpers.accuracy(predict_train, y_train)\n",
    "validation_accuracy = helpers.accuracy(predict_validation, y_validation)\n",
    "\n",
    "print(f\"train_accuracy = {train_accuracy}\")\n",
    "print(f\"validation_accuracy = {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), tx_test]\n",
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Logistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [new_data.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current gamma=1e-06\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=63.90647630054176\n",
      "Current iteration=200, loss=53.02343254839472\n",
      "Current iteration=300, loss=44.368158332960675\n",
      "Current iteration=400, loss=35.82468729084078\n",
      "Current iteration=500, loss=27.30095531012157\n",
      "Current iteration=600, loss=18.81347756457517\n",
      "Current iteration=700, loss=11.95380385074749\n",
      "Current iteration=800, loss=9.503195366473074\n",
      "Current iteration=900, loss=7.751785287349152\n",
      "training_loss = 6.1669070611112256, validation_loss = 9.739648296571119\n",
      "Current gamma=7.498942093324558e-06\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=10.509610190754243\n",
      "Current iteration=200, loss=3.701115962311486\n",
      "Current iteration=300, loss=3.7455438017163165\n",
      "Current iteration=400, loss=3.670723311723105\n",
      "Current iteration=500, loss=3.653082424181691\n",
      "Current iteration=600, loss=3.652305190781663\n",
      "Current iteration=700, loss=3.3775144486288284\n",
      "Current iteration=800, loss=2.507512000503006\n",
      "Current iteration=900, loss=2.8897558963186927\n",
      "training_loss = 3.047229588142272, validation_loss = 4.466476361433559\n",
      "Current gamma=5.623413251903491e-05\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=31.876324038864727\n",
      "Current iteration=200, loss=4.113117108343962\n",
      "Current iteration=300, loss=42.344710961842445\n",
      "Current iteration=400, loss=30.63807277176464\n",
      "Current iteration=500, loss=2.534810177606366\n",
      "Current iteration=600, loss=41.47272218653498\n",
      "Current iteration=700, loss=32.26595899426802\n",
      "Current iteration=800, loss=1.8494362293077253\n",
      "Current iteration=900, loss=41.76642142549732\n",
      "training_loss = 40.76847827329069, validation_loss = -36.892696911982256\n",
      "Current gamma=0.00042169650342858224\n",
      "Current iteration=0, loss=74.99274837100322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=288.5998504558666\n",
      "Current iteration=200, loss=277.4259364801881\n",
      "Current iteration=300, loss=311.6132322804477\n",
      "Current iteration=400, loss=233.80921590042038\n",
      "Current iteration=500, loss=130.36664938852536\n",
      "Current iteration=600, loss=340.08096962468056\n",
      "Current iteration=700, loss=352.2575230792863\n",
      "Current iteration=800, loss=257.32160121572747\n",
      "Current iteration=900, loss=230.92136708567338\n",
      "training_loss = 133.61811692541337, validation_loss = -132.94876036141915\n",
      "Current gamma=0.0031622776601683794\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=1803.5479482008486\n",
      "Current iteration=200, loss=2698.8193117157975\n",
      "Current iteration=300, loss=1850.6770403982462\n",
      "Current iteration=400, loss=2333.647888915868\n",
      "Current iteration=500, loss=878.2525509373368\n",
      "Current iteration=600, loss=1606.302304471584\n",
      "Current iteration=700, loss=1617.6304754495534\n",
      "Current iteration=800, loss=908.8648642171975\n",
      "Current iteration=900, loss=120.91549526963401\n",
      "training_loss = 2758.6421440668587, validation_loss = -2536.5218485894447\n",
      "Current gamma=0.023713737056616554\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=15403.224942138017\n",
      "Current iteration=200, loss=824.7425400884929\n",
      "Current iteration=300, loss=12527.164871123743\n",
      "Current iteration=400, loss=3121.6023145254735\n",
      "Current iteration=500, loss=5947.607336791023\n",
      "Current iteration=600, loss=16341.972192923815\n",
      "Current iteration=700, loss=11379.5018646704\n",
      "Current iteration=800, loss=1652.2421592002208\n",
      "Current iteration=900, loss=14915.067794436834\n",
      "training_loss = 16706.14125504352, validation_loss = -15265.15569367416\n",
      "Current gamma=0.1778279410038923\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=137587.45494151118\n",
      "Current iteration=200, loss=138130.96678896947\n",
      "Current iteration=300, loss=131718.41479861824\n",
      "Current iteration=400, loss=156146.51882894\n",
      "Current iteration=500, loss=89175.00948583528\n",
      "Current iteration=600, loss=53774.58585608675\n",
      "Current iteration=700, loss=128784.8463804457\n",
      "Current iteration=800, loss=100657.69285440666\n",
      "Current iteration=900, loss=134318.60062260914\n",
      "training_loss = 102781.73493417678, validation_loss = -92708.00053273117\n",
      "Current gamma=1.333521432163324\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=1031734.3599336548\n",
      "Current iteration=200, loss=1111806.4231106485\n",
      "Current iteration=300, loss=344885.14915403863\n",
      "Current iteration=400, loss=321037.8646206368\n",
      "Current iteration=500, loss=62798.202864400504\n",
      "Current iteration=600, loss=48938.80712875768\n",
      "Current iteration=700, loss=122740.9170284914\n",
      "Current iteration=800, loss=975748.2190812235\n",
      "Current iteration=900, loss=639489.6580215346\n",
      "training_loss = 308268.0985015996, validation_loss = -306582.6430895334\n",
      "Current gamma=10.0\n",
      "Current iteration=0, loss=74.99274837100322\n",
      "Current iteration=100, loss=7741514.932327581\n",
      "Current iteration=200, loss=5894682.516363727\n",
      "Current iteration=300, loss=8313801.210662053\n",
      "Current iteration=400, loss=5397785.537483745\n",
      "Current iteration=500, loss=7939516.106133644\n",
      "Current iteration=600, loss=6184868.5579700135\n",
      "Current iteration=700, loss=8145501.639071651\n",
      "Current iteration=800, loss=7397109.395408013\n",
      "Current iteration=900, loss=3357158.489083121\n",
      "training_loss = 7821287.60587992, validation_loss = 15534640.376151886\n",
      "Best gamma = 1.333521432163324, training_loss = 308268.0985015996, validation_loss = -306582.6430895334\n",
      "Current lambda=1e-06\n",
      "Current it = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 297314.1500673294, validation_loss = -295925.23013583047\n",
      "Current lambda=0.00031622776601683794\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 229577.1774475597, validation_loss = -228541.58335607703\n",
      "Current lambda=0.1\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 929082.2731807932, validation_loss = -803379.3659650225\n",
      "Best lambda = 0.1, training_loss = 929082.2731807932, validation_loss = -803379.3659650225\n"
     ]
    }
   ],
   "source": [
    "#Find the most optimal values for the regularization term (lambda) and gamma:\n",
    "gamma_opt = hpopt.best_gamma_selection(train_labels, train_data, 1000)\n",
    "lambda_opt = hpopt.best_lambda_selection(train_labels, train_data, 1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n"
     ]
    }
   ],
   "source": [
    "trained_weights, train_loss = impl.reg_logistic_regression(train_labels, new_data, lambda_opt, initial_w, max_iters=1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ...  1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), tx_test]\n",
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_RegLogistics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
