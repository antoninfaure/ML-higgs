{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0118fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "tx_train, y_train, ids_train = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e5e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor the -1 in 0 value for logistic regression\n",
    "y_train[y_train==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d433f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "y_train, tx_train = helpers.shuffle_data(y_train, tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8242a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split, clean and standardize data into 4 sets according to 22nd feature\n",
    "tx_train_0, y_0, _, miss_col_0 = helpers.split_i(tx_train, y_train, ids_train, 0)\n",
    "tx_train_1, y_1, _, miss_col_1 = helpers.split_i(tx_train, y_train, ids_train, 1)\n",
    "tx_train_2, y_2, _, miss_col_2 = helpers.split_i(tx_train, y_train, ids_train, 2)\n",
    "tx_train_3, y_3, _, miss_col_3 = helpers.split_i(tx_train, y_train, ids_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecf9a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tx_train_0.shape)\n",
    "print(tx_train_1.shape)\n",
    "print(tx_train_2.shape)\n",
    "print(tx_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30513e86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=1.223972875168135\n",
      "Current iteration=100, loss=0.510475168028821\n",
      "Current iteration=200, loss=0.453246077889422\n",
      "Current iteration=300, loss=0.4285402418527632\n",
      "Current iteration=400, loss=0.41465919547508046\n",
      "Current iteration=500, loss=0.40558674150795776\n",
      "Current iteration=600, loss=0.39908576146713154\n",
      "Current iteration=700, loss=0.39414552570842004\n",
      "Current iteration=800, loss=0.3902417691006348\n",
      "Current iteration=900, loss=0.3870726471404292\n",
      "Current iteration=1000, loss=0.384448585082879\n",
      "Current iteration=1100, loss=0.38224219253225494\n",
      "Current iteration=1200, loss=0.3803636134428381\n",
      "Current iteration=1300, loss=0.3787471520045637\n",
      "Current iteration=1400, loss=0.37734335638474653\n",
      "Current iteration=1500, loss=0.37611409169068233\n",
      "Current iteration=1600, loss=0.37502940102244536\n",
      "Current iteration=1700, loss=0.3740654414943691\n",
      "Current iteration=1800, loss=0.3732030551617844\n",
      "Current iteration=1900, loss=0.37242672971626406\n",
      "Current iteration=2000, loss=0.37172381915257374\n",
      "Current iteration=2100, loss=0.3710839501981154\n",
      "Current iteration=2200, loss=0.37049856634411826\n",
      "Current iteration=2300, loss=0.3699605751059138\n",
      "Current iteration=2400, loss=0.36946407231182177\n",
      "Current iteration=2500, loss=0.36900412238222036\n",
      "Current iteration=2600, loss=0.36857657786437525\n",
      "Current iteration=2700, loss=0.3681779272622119\n",
      "Current iteration=2800, loss=0.3678051675051578\n",
      "Current iteration=2900, loss=0.36745570214619033\n",
      "Current iteration=3000, loss=0.3671272651260781\n",
      "Current iteration=3100, loss=0.36681786563702756\n",
      "Current iteration=3200, loss=0.3665257474908865\n",
      "Current iteration=3300, loss=0.366249357449835\n",
      "Current iteration=3400, loss=0.36598731912173965\n",
      "Current iteration=3500, loss=0.3657384106860271\n",
      "Current iteration=3600, loss=0.365501545609667\n",
      "Current iteration=3700, loss=0.3652757558859076\n",
      "Current iteration=3800, loss=0.36506017746891534\n",
      "Current iteration=3900, loss=0.3648540379401063\n",
      "Current iteration=4000, loss=0.36465665150975396\n",
      "Current iteration=4100, loss=0.3644674791768753\n",
      "Current iteration=4200, loss=0.364286309300493\n",
      "Current iteration=4300, loss=0.3641128349807621\n",
      "Current iteration=4400, loss=0.363946466787514\n",
      "Current iteration=4500, loss=0.3637866555148684\n",
      "Current iteration=4600, loss=0.3636329351090801\n",
      "Current iteration=4700, loss=0.3634849005446389\n",
      "Current iteration=4800, loss=0.3633421917731283\n",
      "Current iteration=4900, loss=0.36320448443139536\n",
      "Current iteration=0, loss=0.9231250170552501\n",
      "Current iteration=100, loss=0.6132345610063982\n",
      "Current iteration=200, loss=0.5588707775084196\n",
      "Current iteration=300, loss=0.5301226524679085\n",
      "Current iteration=400, loss=0.5124598935770601\n",
      "Current iteration=500, loss=0.500540854186139\n",
      "Current iteration=600, loss=0.49206953466078723\n",
      "Current iteration=700, loss=0.48576394808472906\n",
      "Current iteration=800, loss=0.48086837574223223\n",
      "Current iteration=900, loss=0.47696439554161435\n",
      "Current iteration=1000, loss=0.47378318212099446\n",
      "Current iteration=1100, loss=0.4711442279911372\n",
      "Current iteration=1200, loss=0.46892256829966533\n",
      "Current iteration=1300, loss=0.46702734501442655\n",
      "Current iteration=1400, loss=0.46539069818851153\n",
      "Current iteration=1500, loss=0.46396201843075013\n",
      "Current iteration=1600, loss=0.4627033097277377\n",
      "Current iteration=1700, loss=0.46158556189969724\n",
      "Current iteration=1800, loss=0.46058619576269805\n",
      "Current iteration=1900, loss=0.4596873228071182\n",
      "Current iteration=2000, loss=0.4588745505680343\n",
      "Current iteration=2100, loss=0.45813614256473056\n",
      "Current iteration=2200, loss=0.4574624132283184\n",
      "Current iteration=2300, loss=0.4568452845708051\n",
      "Current iteration=2400, loss=0.4562779572930955\n",
      "Current iteration=2500, loss=0.4557546634789309\n",
      "Current iteration=2600, loss=0.455270477388061\n",
      "Current iteration=2700, loss=0.45482116790847876\n",
      "Current iteration=2800, loss=0.45440308151179765\n",
      "Current iteration=2900, loss=0.45401304818381305\n",
      "Current iteration=3000, loss=0.45364830510620413\n",
      "Current iteration=3100, loss=0.45330643429580336\n",
      "Current iteration=3200, loss=0.4529853113317596\n",
      "Current iteration=3300, loss=0.4526830629442761\n",
      "Current iteration=3400, loss=0.45239803171906523\n",
      "Current iteration=3500, loss=0.45212874654519686\n",
      "Current iteration=3600, loss=0.45187389772775355\n",
      "Current iteration=3700, loss=0.4516323159165088\n",
      "Current iteration=3800, loss=0.4514029541798261\n",
      "Current iteration=3900, loss=0.45118487268976426\n",
      "Current iteration=4000, loss=0.45097722558934134\n",
      "Current iteration=4100, loss=0.45077924969380323\n",
      "Current iteration=4200, loss=0.45059025474074005\n",
      "Current iteration=4300, loss=0.4504096149535358\n",
      "Current iteration=4400, loss=0.45023676172232263\n",
      "Current iteration=4500, loss=0.4500711772386756\n",
      "Current iteration=4600, loss=0.4499123889465138\n",
      "Current iteration=4700, loss=0.4497599646932479\n",
      "Current iteration=4800, loss=0.44961350848312687\n",
      "Current iteration=4900, loss=0.44947265674963344\n",
      "Current iteration=0, loss=1.182543470175275\n",
      "Current iteration=100, loss=0.6291665036205849\n",
      "Current iteration=200, loss=0.558314772136736\n",
      "Current iteration=300, loss=0.5236181160250059\n",
      "Current iteration=400, loss=0.5013547425188373\n",
      "Current iteration=500, loss=0.48523759853287385\n",
      "Current iteration=600, loss=0.4728267028913986\n",
      "Current iteration=700, loss=0.46293173281389866\n",
      "Current iteration=800, loss=0.45485722137429174\n",
      "Current iteration=900, loss=0.44814638101015697\n",
      "Current iteration=1000, loss=0.4424851150308133\n",
      "Current iteration=1100, loss=0.4376489107992025\n",
      "Current iteration=1200, loss=0.4334730862626541\n",
      "Current iteration=1300, loss=0.4298345707865796\n",
      "Current iteration=1400, loss=0.4266394927090775\n",
      "Current iteration=1500, loss=0.4238147551866795\n",
      "Current iteration=1600, loss=0.4213024444647125\n",
      "Current iteration=1700, loss=0.41905602706933104\n",
      "Current iteration=1800, loss=0.4170376666786529\n",
      "Current iteration=1900, loss=0.415216276507505\n",
      "Current iteration=2000, loss=0.41356607630849557\n",
      "Current iteration=2100, loss=0.41206550599219877\n",
      "Current iteration=2200, loss=0.41069639666269303\n",
      "Current iteration=2300, loss=0.40944333052212395\n",
      "Current iteration=2400, loss=0.4082931411703349\n",
      "Current iteration=2500, loss=0.4072345193923326\n",
      "Current iteration=2600, loss=0.4062576989849124\n",
      "Current iteration=2700, loss=0.40535420396216576\n",
      "Current iteration=2800, loss=0.40451664344293653\n",
      "Current iteration=2900, loss=0.4037385441427341\n",
      "Current iteration=3000, loss=0.4030142129538975\n",
      "Current iteration=3100, loss=0.4023386238264693\n",
      "Current iteration=3200, loss=0.4017073243040245\n",
      "Current iteration=3300, loss=0.40111635788051136\n",
      "Current iteration=3400, loss=0.4005621990211947\n",
      "Current iteration=3500, loss=0.4000416983154936\n",
      "Current iteration=3600, loss=0.3995520357936238\n",
      "Current iteration=3700, loss=0.3990906809075089\n",
      "Current iteration=3800, loss=0.3986553580335045\n",
      "Current iteration=3900, loss=0.3982440166112673\n",
      "Current iteration=4000, loss=0.39785480521414746\n",
      "Current iteration=4100, loss=0.3974860489762982\n",
      "Current iteration=4200, loss=0.39713622989850833\n",
      "Current iteration=4300, loss=0.3968039696302265\n",
      "Current iteration=4400, loss=0.39648801438626247\n",
      "Current iteration=4500, loss=0.3961872217072117\n",
      "Current iteration=4600, loss=0.39590054881515635\n",
      "Current iteration=4700, loss=0.3956270423522004\n",
      "Current iteration=4800, loss=0.3953658293199918\n",
      "Current iteration=4900, loss=0.39511610906440253\n",
      "Current iteration=0, loss=1.034153860600359\n",
      "Current iteration=100, loss=0.6505299997145018\n",
      "Current iteration=200, loss=0.5828549538022179\n",
      "Current iteration=300, loss=0.5452516778607739\n",
      "Current iteration=400, loss=0.5208668056460749\n",
      "Current iteration=500, loss=0.5037842665036487\n",
      "Current iteration=600, loss=0.4911150540603392\n",
      "Current iteration=700, loss=0.4813091874166215\n",
      "Current iteration=800, loss=0.4734758377511981\n",
      "Current iteration=900, loss=0.4670739323748523\n",
      "Current iteration=1000, loss=0.4617485331945835\n",
      "Current iteration=1100, loss=0.45725394347187187\n",
      "Current iteration=1200, loss=0.45341378013490524\n",
      "Current iteration=1300, loss=0.45009789770194303\n",
      "Current iteration=1400, loss=0.44720812004949\n",
      "Current iteration=1500, loss=0.44466901236196127\n",
      "Current iteration=1600, loss=0.44242171766277977\n",
      "Current iteration=1700, loss=0.4404197490969091\n",
      "Current iteration=1800, loss=0.4386260796784218\n",
      "Current iteration=1900, loss=0.43701107240058523\n",
      "Current iteration=2000, loss=0.4355508006703739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=2100, loss=0.43422542639761086\n",
      "Current iteration=2200, loss=0.43301793040350234\n",
      "Current iteration=2300, loss=0.4319136040637999\n",
      "Current iteration=2400, loss=0.43089987880208175\n",
      "Current iteration=2500, loss=0.4299660683951964\n",
      "Current iteration=2600, loss=0.42910306015132516\n",
      "Current iteration=2700, loss=0.4283030420671444\n",
      "Current iteration=2800, loss=0.4275592816924598\n",
      "Current iteration=2900, loss=0.42686594843588493\n",
      "Current iteration=3000, loss=0.42621796977353965\n",
      "Current iteration=3100, loss=0.4256109139650078\n",
      "Current iteration=3200, loss=0.4250408937200493\n",
      "Current iteration=3300, loss=0.4245044865833009\n",
      "Current iteration=3400, loss=0.4239986687641739\n",
      "Current iteration=3500, loss=0.42352075985233156\n",
      "Current iteration=3600, loss=0.42306837639912176\n",
      "Current iteration=3700, loss=0.42263939275984735\n",
      "Current iteration=3800, loss=0.422231907913164\n",
      "Current iteration=3900, loss=0.4218442172251153\n",
      "Current iteration=4000, loss=0.42147478832297713\n",
      "Current iteration=4100, loss=0.4211222404005205\n",
      "Current iteration=4200, loss=0.4207853264007726\n",
      "Current iteration=4300, loss=0.4204629176218948\n",
      "Current iteration=4400, loss=0.42015399037177364\n",
      "Current iteration=4500, loss=0.4198576143614854\n",
      "Current iteration=4600, loss=0.41957294258014477\n",
      "Current iteration=4700, loss=0.4192992024362838\n",
      "Current iteration=4800, loss=0.41903568798577256\n",
      "Current iteration=4900, loss=0.4187817530949339\n"
     ]
    }
   ],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w_0 = np.random.normal(0., 0.1, [tx_train_0.shape[1],])\n",
    "initial_w_1 = np.random.normal(0., 0.1, [tx_train_1.shape[1],])\n",
    "initial_w_2 = np.random.normal(0., 0.1, [tx_train_2.shape[1],])\n",
    "initial_w_3 = np.random.normal(0., 0.1, [tx_train_3.shape[1],])\n",
    "\n",
    "# Train models\n",
    "w_0, train_loss_0 = impl.logistic_regression(y_0, tx_train_0, initial_w_0, max_iters=20000, gamma=0.01)\n",
    "w_1, train_loss_1 = impl.logistic_regression(y_1, tx_train_1, initial_w_1, max_iters=20000, gamma=0.01)\n",
    "w_2, train_loss_2 = impl.logistic_regression(y_2, tx_train_2, initial_w_2, max_iters=20000, gamma=0.01)\n",
    "w_3, train_loss_3 = impl.logistic_regression(y_3, tx_train_3, initial_w_3, max_iters=20000, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab257b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss_0 = 0.3630714840319055\n",
      "train_loss_1 = 0.4493370750762544\n",
      "train_loss_2 = 0.3948771461716632\n",
      "train_loss_3 = 0.41853680541110877\n"
     ]
    }
   ],
   "source": [
    "print(f\"train_loss_0 = {train_loss_0}\")\n",
    "print(f\"train_loss_1 = {train_loss_1}\")\n",
    "print(f\"train_loss_2 = {train_loss_2}\")\n",
    "print(f\"train_loss_3 = {train_loss_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bf50be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy_0 = 0.8397205568844895\n",
      "train_accuracy_1 = 0.8004874651810585\n",
      "train_accuracy_2 = 0.8342364874253161\n",
      "train_accuracy_3 = 0.8206099981952716\n"
     ]
    }
   ],
   "source": [
    "# Compute training accuracies\n",
    "predict_train_0 = helpers.predict_logistic(tx_train_0, w_0)\n",
    "predict_train_1 = helpers.predict_logistic(tx_train_1, w_1)\n",
    "predict_train_2 = helpers.predict_logistic(tx_train_2, w_2)\n",
    "predict_train_3 = helpers.predict_logistic(tx_train_3, w_3)\n",
    "\n",
    "predict_train_0[predict_train_0 == -1] = 0\n",
    "predict_train_1[predict_train_1 == -1] = 0\n",
    "predict_train_2[predict_train_2 == -1] = 0\n",
    "predict_train_3[predict_train_3 == -1] = 0\n",
    "\n",
    "train_accuracy_0 = helpers.accuracy(predict_train_0, y_0)\n",
    "train_accuracy_1 = helpers.accuracy(predict_train_1, y_1)\n",
    "train_accuracy_2 = helpers.accuracy(predict_train_2, y_2)\n",
    "train_accuracy_3 = helpers.accuracy(predict_train_3, y_3)\n",
    "\n",
    "print(f\"train_accuracy_0 = {train_accuracy_0}\")\n",
    "print(f\"train_accuracy_1 = {train_accuracy_1}\")\n",
    "print(f\"train_accuracy_2 = {train_accuracy_2}\")\n",
    "print(f\"train_accuracy_3 = {train_accuracy_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5193e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d8c7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split, clean, standardize, expand degree and add bias to data into 4 sets according to 22nd feature\n",
    "tx_test_0, y_test_0, ids_test_0, _ = helpers.split_i(tx_test, y_test, ids_test, 0, miss_col_0)\n",
    "tx_test_1, y_test_1, ids_test_1, _ = helpers.split_i(tx_test, y_test, ids_test, 1, miss_col_1)\n",
    "tx_test_2, y_test_2, ids_test_2, _ = helpers.split_i(tx_test, y_test, ids_test, 2, miss_col_2)\n",
    "tx_test_3, y_test_3, ids_test_3, _ = helpers.split_i(tx_test, y_test, ids_test, 3, miss_col_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba8668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels\n",
    "predict_test_0 = helpers.predict_logistic(tx_test_0, w_0)\n",
    "predict_test_1 = helpers.predict_logistic(tx_test_1, w_1)\n",
    "predict_test_2 = helpers.predict_logistic(tx_test_2, w_2)\n",
    "predict_test_3 = helpers.predict_logistic(tx_test_3, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04bd7443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1. -1. -1.]\n",
      "[-1. -1. -1. ... -1.  1.  1.]\n",
      "[-1.  1. -1. ... -1. -1. -1.]\n",
      "[-1.  1. -1. ... -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "print(predict_test_0)\n",
    "print(predict_test_1)\n",
    "print(predict_test_2)\n",
    "print(predict_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e6f643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets\n",
    "predict_test = np.concatenate((predict_test_0, predict_test_1, predict_test_2, predict_test_3))\n",
    "ids_test = np.concatenate((ids_test_0, ids_test_1, ids_test_2, ids_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f84a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50c95d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Logistics_degree2_split4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ba3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
