{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0118fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "tx_train, y_train, ids_train = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e5e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor the -1 in 0 value for logistic regression\n",
    "y_train[y_train==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d433f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "y_train, tx_train = helpers.shuffle_data(y_train, tx_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8242a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean data into 4 sets according to 22nd feature\n",
    "tx_train_0, y_0, _, miss_col_0 = helpers.split_i(tx_train, y_train, ids_train, 0)\n",
    "tx_train_1, y_1, _, miss_col_1 = helpers.split_i(tx_train, y_train, ids_train, 1)\n",
    "tx_train_2, y_2, _, miss_col_2 = helpers.split_i(tx_train, y_train, ids_train, 2)\n",
    "tx_train_3, y_3, _, miss_col_3 = helpers.split_i(tx_train, y_train, ids_train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4e7ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "tx_train_0, mean_0, std_0 = helpers.standardize(tx_train_0)\n",
    "tx_train_1, mean_1, std_1 = helpers.standardize(tx_train_1)\n",
    "tx_train_2, mean_2, std_2 = helpers.standardize(tx_train_2)\n",
    "tx_train_3, mean_3, std_3 = helpers.standardize(tx_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94041bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to degree 2\n",
    "tx_train_0 = helpers.build_poly_deg2(tx_train_0)\n",
    "tx_train_1 = helpers.build_poly_deg2(tx_train_1)\n",
    "tx_train_2 = helpers.build_poly_deg2(tx_train_2)\n",
    "tx_train_3 = helpers.build_poly_deg2(tx_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a12bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_train_0 = np.c_[np.ones((tx_train_0.shape[0], 1)), tx_train_0]\n",
    "tx_train_1 = np.c_[np.ones((tx_train_1.shape[0], 1)), tx_train_1]\n",
    "tx_train_2 = np.c_[np.ones((tx_train_2.shape[0], 1)), tx_train_2]\n",
    "tx_train_3 = np.c_[np.ones((tx_train_3.shape[0], 1)), tx_train_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ecf9a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99913, 190)\n",
      "(77544, 276)\n",
      "(50379, 465)\n",
      "(22164, 465)\n"
     ]
    }
   ],
   "source": [
    "print(tx_train_0.shape)\n",
    "print(tx_train_1.shape)\n",
    "print(tx_train_2.shape)\n",
    "print(tx_train_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f9c29640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_gd(y, tx, w_init, max_iters, gamma):\n",
    "    \"\"\"Training function to compute mean squared error solution using gradient descent\n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Labels of shape (N,), N is the number of samples.\n",
    "        tx (np.array): Dataset of shape (N,D), D is the number of features.\n",
    "        w_init (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (float): Step size\n",
    "        \n",
    "    Returns:\n",
    "        w (np.array): optimal weights, numpy array of shape(D,), D is the number of features.\n",
    "        loss (float): Final value of the cost function.\n",
    "    \"\"\"\n",
    "    w = w_init.copy()\n",
    "    for n in range (max_iters):\n",
    "        loss = np.sum(np.square(tx.dot(w) - y, dtype=float), dtype=float)/(2*tx.shape[0])\n",
    "        gradient = np.dot(tx.dot(w) - y, tx)/tx.shape[0]\n",
    "        print(gradient.mean())\n",
    "        w += gamma * gradient\n",
    "        # Log info\n",
    "        if n % 100 == 0:\n",
    "            print(f\"Current iteration={n}, loss={loss}\")\n",
    "    loss = np.sum(np.square(tx.dot(w) - y))/(2*tx.shape[0])\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30513e86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-74.05522338571619\n",
      "Current iteration=0, loss=411.76001092343887\n",
      "293399.04547187936\n",
      "-1156020536.8112617\n",
      "4554374293578.085\n",
      "-1.7942842337294204e+16\n",
      "7.068931171597252e+19\n",
      "-2.784942706038226e+23\n",
      "1.0971822596098242e+27\n",
      "-4.3225625726244665e+30\n",
      "1.7029574649610666e+34\n",
      "-6.709131629078555e+37\n",
      "2.643192689333022e+41\n",
      "-1.0413370878972057e+45\n",
      "4.102549674136178e+48\n",
      "-1.616279111189807e+52\n",
      "6.367645422400747e+55\n",
      "-2.5086575669206566e+59\n",
      "9.88334363896714e+62\n",
      "-3.8937351503822215e+66\n",
      "1.534012574605415e+70\n",
      "-6.043540426257617e+73\n",
      "2.380970109922683e+77\n",
      "-9.380294106604876e+80\n",
      "3.6955490184319067e+84\n",
      "-1.455933299363916e+88\n",
      "5.735931959295529e+91\n",
      "-2.2597817809402506e+95\n",
      "8.90284915111973e+98\n",
      "-3.507450306755539e+102\n",
      "1.3818281592261122e+106\n",
      "-5.443980369308496e+109\n",
      "2.1447617826817303e+113\n",
      "-8.449705532344571e+116\n",
      "3.3289255785815994e+120\n",
      "-1.3114948758054445e+124\n",
      "5.166888741312101e+127\n",
      "-2.035596155013748e+131\n",
      "8.019626343365866e+134\n",
      "-3.1594875304121145e+138\n",
      "1.244741466425984e+142\n",
      "-4.9039007222738717e+145\n",
      "1.9319869179716216e+149\n",
      "-7.611437634248727e+152\n",
      "2.998673661873551e+156\n",
      "-1.1813857200843601e+160\n",
      "4.6542984565623794e+163\n",
      "-1.8336512583893755e+167\n",
      "7.224025207606136e+170\n",
      "-2.846045013268672e+174\n",
      "1.1212547000836977e+178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_1357/3616894931.py:17: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.sum(np.square(tx.dot(w) - y, dtype=float), dtype=float)/(2*tx.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.417400626478098e+181\n",
      "1.7403207579288123e+185\n",
      "-6.85633157727108e+188\n",
      "2.70118496738678e+192\n",
      "-1.064184271983588e+196\n",
      "4.192560592519713e+199\n",
      "-1.6517406604013806e+203\n",
      "6.507353081767882e+206\n",
      "-2.5636981122995275e+210\n",
      "1.0100186555764188e+214\n",
      "-3.979164628308659e+217\n",
      "1.567669176382308e+221\n",
      "-6.1761371447046535e+224\n",
      "2.4332091620392877e+228\n",
      "-9.586099996675264e+231\n",
      "3.7766302453522244e+235\n",
      "-1.487876823218623e+239\n",
      "5.861779674607035e+242\n",
      "-2.309361932213356e+246\n",
      "9.098179784987645e+249\n",
      "-3.584404603076721e+253\n",
      "1.412145798630761e+257\n",
      "-5.563422597099737e+260\n",
      "2.191818367758573e+264\n",
      "-8.635094087133068e+267\n",
      "3.401962999785103e+271\n",
      "-1.3402693861960294e+275\n",
      "5.280251512693626e+278\n",
      "-2.0802576201815176e+282\n",
      "8.195578858166407e+285\n",
      "-3.2288074404247763e+289\n",
      "1.2720513910929325e+293\n",
      "-5.011493473790351e+296\n",
      "1.974375171766036e+300\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/numpy/core/_methods.py:180: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Current iteration=100, loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Current iteration=200, loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "Current iteration=300, loss=nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m initial_w_3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(\u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, [tx_train_3\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],])\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Train models\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#w_0, train_loss_0 = impl.logistic_regression(y_0, tx_train_0, initial_w_0, max_iters=10000, gamma=0.1)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#w_1, train_loss_1 = impl.logistic_regression(y_1, tx_train_1, initial_w_1, max_iters=10000, gamma=0.01)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#w_2, train_loss_2 = impl.logistic_regression(y_2, tx_train_2, initial_w_2, max_iters=10000, gamma=0.01)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#w_3, train_loss_3 = impl.logistic_regression(y_3, tx_train_3, initial_w_3, max_iters=10000, gamma=0.01)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m w_0, train_loss_0 \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error_gd\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx_train_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m w_1, train_loss_1 \u001b[38;5;241m=\u001b[39m impl\u001b[38;5;241m.\u001b[39mmean_squared_error_gd(y_1, tx_train_1, initial_w_1, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     14\u001b[0m w_2, train_loss_2 \u001b[38;5;241m=\u001b[39m impl\u001b[38;5;241m.\u001b[39mmean_squared_error_gd(y_2, tx_train_2, initial_w_2, max_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mmean_squared_error_gd\u001b[0;34m(y, tx, w_init, max_iters, gamma)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (max_iters):\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39msquare(tx\u001b[38;5;241m.\u001b[39mdot(w) \u001b[38;5;241m-\u001b[39m y, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39mtx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(gradient\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     20\u001b[0m     w \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m gamma \u001b[38;5;241m*\u001b[39m gradient\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w_0 = np.random.normal(0., 0.1, [tx_train_0.shape[1],])\n",
    "initial_w_1 = np.random.normal(0., 0.1, [tx_train_1.shape[1],])\n",
    "initial_w_2 = np.random.normal(0., 0.1, [tx_train_2.shape[1],])\n",
    "initial_w_3 = np.random.normal(0., 0.1, [tx_train_3.shape[1],])\n",
    "\n",
    "# Train models\n",
    "#w_0, train_loss_0 = impl.logistic_regression(y_0, tx_train_0, initial_w_0, max_iters=10000, gamma=0.1)\n",
    "#w_1, train_loss_1 = impl.logistic_regression(y_1, tx_train_1, initial_w_1, max_iters=10000, gamma=0.01)\n",
    "#w_2, train_loss_2 = impl.logistic_regression(y_2, tx_train_2, initial_w_2, max_iters=10000, gamma=0.01)\n",
    "#w_3, train_loss_3 = impl.logistic_regression(y_3, tx_train_3, initial_w_3, max_iters=10000, gamma=0.01)\n",
    "w_0, train_loss_0 = mean_squared_error_gd(y_0, tx_train_0, initial_w_0, max_iters=15000, gamma=0.1)\n",
    "w_1, train_loss_1 = impl.mean_squared_error_gd(y_1, tx_train_1, initial_w_1, max_iters=15000, gamma=0.01)\n",
    "w_2, train_loss_2 = impl.mean_squared_error_gd(y_2, tx_train_2, initial_w_2, max_iters=15000, gamma=0.01)\n",
    "w_3, train_loss_3 = impl.mean_squared_error_gd(y_3, tx_train_3, initial_w_3, max_iters=15000, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab257b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_loss_0 = {train_loss_0}\")\n",
    "print(f\"train_loss_1 = {train_loss_1}\")\n",
    "print(f\"train_loss_2 = {train_loss_2}\")\n",
    "print(f\"train_loss_3 = {train_loss_3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d895c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tx, w):\n",
    "    y = tx.dot(w)\n",
    "    y[y < 0.5] = -1\n",
    "    y[y >= 0.5] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf50be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training accuracies\n",
    "#predict_train_0 = helpers.predict_logistic(tx_train_0, w_0)\n",
    "#predict_train_1 = helpers.predict_logistic(tx_train_1, w_1)\n",
    "#predict_train_2 = helpers.predict_logistic(tx_train_2, w_2)\n",
    "#predict_train_3 = helpers.predict_logistic(tx_train_3, w_3)\n",
    "predict_train_0 = predict(tx_train_0, w_0)\n",
    "predict_train_1 = predict(tx_train_1, w_1)\n",
    "predict_train_2 = predict(tx_train_2, w_2)\n",
    "predict_train_3 = predict(tx_train_3, w_3)\n",
    "\n",
    "predict_train_0[predict_train_0 == -1] = 0\n",
    "predict_train_1[predict_train_1 == -1] = 0\n",
    "predict_train_2[predict_train_2 == -1] = 0\n",
    "predict_train_3[predict_train_3 == -1] = 0\n",
    "\n",
    "predict_train = np.concatenate((predict_train_0, predict_train_1, predict_train_2, predict_train_3))\n",
    "\n",
    "train_accuracy_0 = helpers.accuracy(predict_train_0, y_0)\n",
    "train_accuracy_1 = helpers.accuracy(predict_train_1, y_1)\n",
    "train_accuracy_2 = helpers.accuracy(predict_train_2, y_2)\n",
    "train_accuracy_3 = helpers.accuracy(predict_train_3, y_3)\n",
    "train_accuracy = helpers.accuracy(predict_train, np.concatenate((y_0, y_1, y_2, y_3)))\n",
    "\n",
    "print(f\"train_accuracy_0 = {train_accuracy_0}\")\n",
    "print(f\"train_accuracy_1 = {train_accuracy_1}\")\n",
    "print(f\"train_accuracy_2 = {train_accuracy_2}\")\n",
    "print(f\"train_accuracy_3 = {train_accuracy_3}\")\n",
    "print(f\"train_accuracy = {train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1079a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5193e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and clean data into 4 sets according to 22nd feature\n",
    "tx_test_0, y_test_0, ids_test_0, _ = helpers.split_i(tx_test, y_test, ids_test, 0, miss_col_0)\n",
    "tx_test_1, y_test_1, ids_test_1, _ = helpers.split_i(tx_test, y_test, ids_test, 1, miss_col_1)\n",
    "tx_test_2, y_test_2, ids_test_2, _ = helpers.split_i(tx_test, y_test, ids_test, 2, miss_col_2)\n",
    "tx_test_3, y_test_3, ids_test_3, _ = helpers.split_i(tx_test, y_test, ids_test, 3, miss_col_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ea7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "tx_test_0, _, _ = helpers.standardize(tx_test_0, mean_0, std_0)\n",
    "tx_test_1, _, _ = helpers.standardize(tx_test_1, mean_1, std_1)\n",
    "tx_test_2, _, _ = helpers.standardize(tx_test_2, mean_2, std_2)\n",
    "tx_test_3, _, _ = helpers.standardize(tx_test_3, mean_3, std_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bc92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to degree 2\n",
    "tx_test_0 = helpers.build_poly_deg2(tx_test_0)\n",
    "tx_test_1 = helpers.build_poly_deg2(tx_test_1)\n",
    "tx_test_2 = helpers.build_poly_deg2(tx_test_2)\n",
    "tx_test_3 = helpers.build_poly_deg2(tx_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_test_0 = np.c_[np.ones((tx_test_0.shape[0], 1)), tx_test_0]\n",
    "tx_test_1 = np.c_[np.ones((tx_test_1.shape[0], 1)), tx_test_1]\n",
    "tx_test_2 = np.c_[np.ones((tx_test_2.shape[0], 1)), tx_test_2]\n",
    "tx_test_3 = np.c_[np.ones((tx_test_3.shape[0], 1)), tx_test_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8668dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels\n",
    "#predict_test_0 = helpers.predict_logistic(tx_test_0, w_0)\n",
    "#predict_test_1 = helpers.predict_logistic(tx_test_1, w_1)\n",
    "#predict_test_2 = helpers.predict_logistic(tx_test_2, w_2)\n",
    "#predict_test_3 = helpers.predict_logistic(tx_test_3, w_3)\n",
    "predict_test_0 = predict(tx_test_0, w_0)\n",
    "predict_test_1 = predict(tx_test_1, w_1)\n",
    "predict_test_2 = predict(tx_test_2, w_2)\n",
    "predict_test_3 = predict(tx_test_3, w_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd7443",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_test_0)\n",
    "print(predict_test_1)\n",
    "print(predict_test_2)\n",
    "print(predict_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sets\n",
    "predict_test = np.concatenate((predict_test_0, predict_test_1, predict_test_2, predict_test_3))\n",
    "ids_test = np.concatenate((ids_test_0, ids_test_1, ids_test_2, ids_test_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f84a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c95d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv file\n",
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Ridge_Regression.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ba3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
