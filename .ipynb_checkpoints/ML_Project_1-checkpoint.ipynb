{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbf5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38c24069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path_dataset,sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", dtype=str,  skip_header=1)\n",
    "    labels = data[:,1]\n",
    "    labels[labels=='s']=0\n",
    "    labels[labels=='b']=1\n",
    "    labels = np.asarray(labels, dtype=float)\n",
    "    data = np.delete(data, 1, 1)\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    #data[:,:][data[:,:]==-999]=None\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f3825",
   "metadata": {},
   "source": [
    "Prediction of :\n",
    "- type s is assigned value 0\n",
    "- type b is assigned value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0118fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, labels = load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8de226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns containing a -999 value\n",
    "valid_cols = np.all(train_data!=-999, axis=0)\n",
    "train_data = train_data[:,valid_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da96a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "new_data = (train_data - train_data.min(axis=0)) / train_data.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "725b7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 7.48541825e-02 6.78088948e-02 ... 1.22284399e-01\n",
      "  6.66666667e-01 6.94837193e-02]\n",
      " [2.85715102e-06 9.96529363e-02 7.18167475e-02 ... 7.52843347e-02\n",
      "  3.33333333e-01 2.82999058e-02]\n",
      " [5.71430204e-06 2.35006340e-01 8.86529895e-02 ... 1.23123231e-01\n",
      "  3.33333333e-01 2.70907959e-02]\n",
      " ...\n",
      " [7.14279184e-01 8.77093070e-02 5.15136536e-02 ... 9.24307477e-02\n",
      "  3.33333333e-01 2.57078191e-02]\n",
      " [7.14282041e-01 2.80578198e-02 4.63059649e-02 ... 4.94217496e-02\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [7.14284898e-01 1.05432018e-01 4.78022397e-02 ... 4.27784564e-02\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[0.7142849  1.         0.9953096  1.         0.96340605 1.\n",
      " 0.97511204 0.99762302 2.         0.97383596 2.00080096 2.\n",
      " 0.95359389 2.00079904 2.         0.99996166 2.         0.99317457\n",
      " 1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(new_data)\n",
    "print(np.max(new_data, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781599cb",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "- data (np.array): Dataset of shape (N, D) \n",
    "- labels (np.array): Labels of shape (N, ) \n",
    "- w (np.array): Weights of logistic regression model of shape (D, ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d798e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_logistic(data, labels, w): \n",
    "    \"\"\" Logistic regression loss function for binary classes\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Dataset of shape (N, D).\n",
    "        labels (np.array): Labels of shape (N, ).\n",
    "        w (np.array): Weights of logistic regression model of shape (D, )\n",
    "    Returns:\n",
    "        int: Loss of logistic regression.\n",
    "    \"\"\"    \n",
    "    return -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5abde076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1/(1+np.exp(-t))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b1de1056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Dataset of shape (N, D).\n",
    "        tx (np.array): Labels of shape (N, ).\n",
    "        initial_w (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (integer): Step size\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"  \n",
    "    w = initial_w.copy()\n",
    "    for it in range(max_iters):\n",
    "        # Compute gradient\n",
    "        gradient = y.T.dot(sigmoid(y.dot(w)) - tx)\n",
    "        w = w - gamma * gradient\n",
    "        # Classify the predictions\n",
    "        predictions = sigmoid(y @ w)\n",
    "        print(predictions)\n",
    "        predictions[predictions<0.5]=0\n",
    "        predictions[predictions>=0.5]=1\n",
    "        # Compute loss\n",
    "        loss = loss_logistic(y, tx, w)\n",
    "        print('loss at iteration', it)\n",
    "        # Compute accuracy\n",
    "        accuracy = np.sum(tx == predictions) / tx.shape[0];\n",
    "        print(accuracy)\n",
    "        # If accurate then break\n",
    "        if accuracy == 1:\n",
    "            break\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "69d71188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 0\n",
      "0.657332\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 1\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 2\n",
      "0.657332\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 3\n",
      "0.657068\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 4\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 5\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 6\n",
      "0.660044\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 7\n",
      "0.499924\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_16501/3627643273.py:11: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))\n",
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_16501/3627643273.py:11: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))\n",
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_16501/1698253809.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iteration 8\n",
      "0.657332\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 9\n",
      "0.55682\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 10\n",
      "0.657332\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 11\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 12\n",
      "0.657332\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 13\n",
      "0.656784\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 14\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 15\n",
      "0.657316\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 16\n",
      "0.661004\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 17\n",
      "0.632956\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 18\n",
      "0.656988\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 19\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 20\n",
      "0.657332\n",
      "[1.00000000e+00 0.00000000e+00 0.00000000e+00 ... 3.03943721e-32\n",
      " 0.00000000e+00 1.00000000e+00]\n",
      "loss at iteration 21\n",
      "0.661084\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 22\n",
      "0.57948\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 23\n",
      "0.657328\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 24\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 25\n",
      "0.657332\n",
      "[1. 1. 0. ... 1. 1. 1.]\n",
      "loss at iteration 26\n",
      "0.657584\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 27\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 28\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 29\n",
      "0.661272\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 30\n",
      "0.591188\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 31\n",
      "0.656964\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 32\n",
      "0.342668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 33\n",
      "0.657332\n",
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 1.00000000e+000\n",
      " 6.66266086e-219 1.00000000e+000]\n",
      "loss at iteration 34\n",
      "0.661168\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 35\n",
      "0.561144\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 36\n",
      "0.657256\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 37\n",
      "0.342672\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 38\n",
      "0.657332\n",
      "[1. 1. 0. ... 1. 1. 1.]\n",
      "loss at iteration 39\n",
      "0.658696\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 40\n",
      "0.34648\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 41\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 42\n",
      "0.661116\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 43\n",
      "0.579012\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 44\n",
      "0.65666\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 45\n",
      "0.342672\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 46\n",
      "0.657328\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 47\n",
      "0.662984\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 48\n",
      "0.619108\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 49\n",
      "0.656964\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 50\n",
      "0.342672\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 51\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 52\n",
      "0.663256\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 53\n",
      "0.588888\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 54\n",
      "0.65732\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 55\n",
      "0.342672\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 56\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 57\n",
      "0.66034\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 58\n",
      "0.344372\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 59\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 60\n",
      "0.66048\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 61\n",
      "0.529464\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 62\n",
      "0.657024\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 63\n",
      "0.473368\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 64\n",
      "0.657312\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 65\n",
      "0.63234\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 66\n",
      "0.656836\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 67\n",
      "0.342676\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 68\n",
      "0.657332\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 2.39086523e-17\n",
      " 0.00000000e+00 1.00000000e+00]\n",
      "loss at iteration 69\n",
      "0.663844\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 70\n",
      "0.619272\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 71\n",
      "0.65732\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 72\n",
      "0.342676\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 73\n",
      "0.657332\n",
      "[1.00000000e+000 0.00000000e+000 0.00000000e+000 ... 1.00000000e+000\n",
      " 3.46292352e-249 1.00000000e+000]\n",
      "loss at iteration 74\n",
      "0.663088\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 75\n",
      "0.354128\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 76\n",
      "0.657332\n",
      "[1. 1. 0. ... 1. 1. 1.]\n",
      "loss at iteration 77\n",
      "0.658064\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 78\n",
      "0.348788\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 79\n",
      "0.657308\n",
      "[1. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 80\n",
      "0.663832\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 81\n",
      "0.622532\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 82\n",
      "0.65684\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 83\n",
      "0.342692\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 84\n",
      "0.657328\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 85\n",
      "0.665232\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 86\n",
      "0.637668\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 87\n",
      "0.657084\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 88\n",
      "0.342684\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 89\n",
      "0.657332\n",
      "[0. 0. 0. ... 1. 0. 1.]\n",
      "loss at iteration 90\n",
      "0.666416\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 91\n",
      "0.609908\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 92\n",
      "0.657332\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 93\n",
      "0.342692\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 94\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 95\n",
      "0.6621\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "loss at iteration 96\n",
      "0.343056\n",
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "loss at iteration 97\n",
      "0.657332\n",
      "[1. 0. 0. ... 1. 1. 1.]\n",
      "loss at iteration 98\n",
      "0.662484\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "loss at iteration 99\n",
      "0.485268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  4705.2104046 ,  16703.07283191,    942.35780362,  -1264.55135283,\n",
       "         -1722.98168734,    694.90569956,  -2596.65588867,  10690.12027033,\n",
       "        -20295.0237282 ,  -6305.08141304,    117.4222028 ,    735.07061205,\n",
       "          2696.52694333,   -382.14153526,   -395.20387057,    585.65849812,\n",
       "         -2367.15214072,   -353.08558476,    558.56921426,   -917.80731127]),\n",
       " nan)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "initial_w = np.random.normal(0., 0.1, [new_data.shape[1],])\n",
    "logistic_regression(new_data, labels, initial_w, max_iters=100, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914e0a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18478c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
