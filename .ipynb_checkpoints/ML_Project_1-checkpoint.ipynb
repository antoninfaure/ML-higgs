{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cbf5f984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "#import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38c24069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train(path_dataset,sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", dtype=str,  skip_header=1)\n",
    "    ids = data[:,0]\n",
    "    labels = data[:,1]\n",
    "    labels[labels=='s']=0\n",
    "    labels[labels=='b']=1\n",
    "    labels = np.asarray(labels, dtype=float)\n",
    "    data = np.delete(data, [0,1], 1)\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bae6bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(path_dataset,sub_sample=True, add_outlier=False):\n",
    "    \"\"\"Load data and convert it to the metric system.\"\"\"\n",
    "    data = np.genfromtxt(\n",
    "        path_dataset, delimiter=\",\", dtype=str,  skip_header=1)\n",
    "    ids = data[:,0]\n",
    "    labels = data[:,1]\n",
    "    data = np.delete(data, [0,1], 1)\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    return data, labels, ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888f3825",
   "metadata": {},
   "source": [
    "Prediction of :\n",
    "- type s is assigned value 0\n",
    "- type b is assigned value 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0118fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, train_labels, ids = load_data_train('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11ed6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    # Remove columns with more than 50% of -999\n",
    "    dirty_cols = np.where(np.sum(data == -999, axis=0)/data.shape[0] < 0.5, True, False)\n",
    "    data = data[:, dirty_cols]\n",
    "    # Replace -999 by nan\n",
    "    data = np.where(data == -999, np.nan, data)\n",
    "    # Compute the columns means without nan values \n",
    "    means = np.nanmean(data, axis=0)\n",
    "    #Find indices that you need to replace\n",
    "    inds = np.where(np.isnan(data))\n",
    "    #Place column means in the indices. Align the arrays using take\n",
    "    data[inds] = np.take(means, inds[1])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8de226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_old(data):\n",
    "    # remove the columns containing a -999 value\n",
    "    valid_cols = np.all(data!=-999, axis=0)\n",
    "    return data[:,valid_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7fb4bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "def standardize(x):\n",
    "    #Standardize the original data set.\n",
    "    mean_x = np.mean(x, axis=0)\n",
    "    x = x - mean_x\n",
    "    std_x = np.std(x, axis=0)\n",
    "    x = x / std_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "19e5e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = standardize(clean_data(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abdd6156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.14910656e-01  6.83319669e-02  4.07680272e-01 ...  1.55729751e+00\n",
      "   3.24824359e-01  4.12510497e-01]\n",
      " [ 7.40827026e-01  5.52504823e-01  5.40136414e-01 ...  5.26704866e-01\n",
      "   8.32993155e-01 -2.73819964e-01]\n",
      " [ 0.00000000e+00  3.19515553e+00  1.09655998e+00 ...  1.48714489e+00\n",
      "  -1.43454996e+00 -2.93969845e-01]\n",
      " ...\n",
      " [-3.10930673e-01  3.19316447e-01 -1.30863670e-01 ...  1.30416949e+00\n",
      "  -1.09325452e-01 -3.17017229e-01]\n",
      " [-5.10097335e-01 -8.45323970e-01 -3.02973380e-01 ...  1.00367341e-17\n",
      "   1.11117522e-17 -7.45439413e-01]\n",
      " [ 0.00000000e+00  6.65336083e-01 -2.53522760e-01 ...  1.00367341e-17\n",
      "   1.11117522e-17 -7.45439413e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781599cb",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "- data (np.array): Dataset of shape (N, D) \n",
    "- labels (np.array): Labels of shape (N, ) \n",
    "- w (np.array): Weights of logistic regression model of shape (D, ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d759b",
   "metadata": {},
   "source": [
    "Recall that the cross entropy loss is defined as:\n",
    "$$\n",
    "L(w) = -\\sum_i (y_i \\log(\\hat{y}(x_i)) + (1-y_i)\\log(1-\\hat{y}(x_i)))\n",
    "$$\n",
    "and vectorized :\n",
    "$$\n",
    "L(w) = -y^T\\log(\\hat{y}(x)-(1-y)^T\\log(1-\\hat{y}(x))$$\n",
    "\n",
    "$$\n",
    "\\delta L(w) = \\sum (\\hat{y}(x_i) - y_i)x_i    ???\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta L(w) = x^T(\\hat{y}(x)-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "8e91de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_reg(labels, data, w, lambda_):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = np.sum(np.logaddexp(0, data @ w) + labels * data.dot(w)) + lambda_*np.linalg.norm(w)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8203b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "816cd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_old(labels, data, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = -np.sum(labels * np.log(sigmoid(data @ w)) + (1 - labels) * np.log(1 - sigmoid(data @ w)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "412a5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(labels, data, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    loss = np.sum(np.logaddexp(0, data @ w) + labels * data.dot(w))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9be7b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Labels of shape (N, ).\n",
    "        tx (np.array): Dataset of shape (N, D).\n",
    "        initial_w (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (integer): Step size\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"  \n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    \n",
    "    w = initial_w.copy()\n",
    "    for it in range(max_iters):\n",
    "        #loss = np.sum(np.logaddexp(0, tx.dot(w)) + y * tx.dot(w))\n",
    "        grad = tx.T.dot(sigmoid(tx.dot(w)) - y)\n",
    "        w -= gamma * grad\n",
    "        # log info\n",
    "        if it % 100 == 0:\n",
    "            print(f\"Current iteration={it}\")\n",
    "        #    print(\"Current iteration={i}, loss={l}\".format(i=it, l=loss))\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            #break\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    print(\"loss={l}\".format(l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "744339be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \"\"\" Training function for binary class logistic regression. \n",
    "    \n",
    "    Args:\n",
    "        y (np.array): Labels of shape (N, ).\n",
    "        tx (np.array): Dataset of shape (N, D).\n",
    "        lambda_ (integer): Regularization factor\n",
    "        initial_w (np.array): Initial weights of shape (D,)\n",
    "        max_iters (integer): Maximum number of iterations.\n",
    "        gamma (integer): Step size\n",
    "    Returns:\n",
    "        np.array: weights of shape(D, )\n",
    "    \"\"\"  \n",
    "    def sigmoid(t):\n",
    "        \"\"\"apply sigmoid function on t.\"\"\"\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "    \n",
    "    w = initial_w.copy()\n",
    "    for it in range(max_iters):\n",
    "        #loss = -np.sum(y * np.log(sigmoid(tx @ w)) + (1 - y) * np.log(1 - sigmoid(tx @ w)))\n",
    "        grad = tx.T.dot(sigmoid(tx.dot(w)) - y) + 2*lambda_*w\n",
    "        w -= gamma * grad\n",
    "        # log info\n",
    "        if it % 100 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=it, l=loss))\n",
    "            print(f\"Current iteration={it}\")\n",
    "        # converge criterion\n",
    "        #losses.append(loss)\n",
    "        #if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "        #    break\n",
    "    loss = calculate_loss_reg(y, tx, w, lambda_)\n",
    "    print(\"loss={l}\".format(l=loss))\n",
    "    return (w, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30513e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_1224/3317891671.py:16: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100\n",
      "Current iteration=200\n",
      "Current iteration=300\n",
      "Current iteration=400\n",
      "Current iteration=500\n",
      "Current iteration=600\n",
      "Current iteration=700\n",
      "Current iteration=800\n",
      "Current iteration=900\n",
      "Current iteration=1000\n",
      "Current iteration=1100\n",
      "Current iteration=1200\n",
      "Current iteration=1300\n",
      "Current iteration=1400\n",
      "Current iteration=1500\n",
      "Current iteration=1600\n",
      "Current iteration=1700\n",
      "Current iteration=1800\n",
      "Current iteration=1900\n",
      "Current iteration=2000\n",
      "Current iteration=2100\n",
      "Current iteration=2200\n",
      "Current iteration=2300\n",
      "Current iteration=2400\n",
      "Current iteration=2500\n",
      "Current iteration=2600\n",
      "Current iteration=2700\n",
      "Current iteration=2800\n",
      "Current iteration=2900\n",
      "Current iteration=3000\n",
      "Current iteration=3100\n",
      "Current iteration=3200\n"
     ]
    }
   ],
   "source": [
    "# Add bias to data\n",
    "tx = np.c_[np.ones((train_labels.shape[0], 1)), new_data]\n",
    "#initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [tx.shape[1],])\n",
    "#trained_weights, train_loss = logistic_regression(train_labels, tx, initial_w, max_iters=4000, gamma=0.01)\n",
    "trained_weights, train_loss = reg_logistic_regression(train_labels, tx, 1, initial_w, max_iters=4000, gamma=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a8a57525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_logistic(tx, w):\n",
    "    def sigmoid(t):\n",
    "        return 1.0 / (1 + np.exp(-t))\n",
    "    y = sigmoid(tx @ w)\n",
    "    # s = 1 , b = -1\n",
    "    y[y < 0.5] = 1\n",
    "    y[y >= 0.5] = -1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9288e049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  823.02512847   -79.17503719   566.57877195  1178.9373725\n",
      " -1308.13841605  -562.7990073     46.29895111  -141.42865866\n",
      "   724.74861699  -469.00635392  -816.3652398      3.34907989\n",
      "    -7.43860355 -1206.85660937   -11.79690743    -3.12230748\n",
      "  -476.48465348    -6.88608745  -330.97660713  -825.02958601\n",
      "  -264.4015371     -3.10177048     2.17331447   291.38871187]\n"
     ]
    }
   ],
   "source": [
    "print(trained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7bf50be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 ... -1  1  1]\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_1224/3997087182.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "tx_validation = np.c_[np.ones((train_labels.shape[0], 1)), new_data]\n",
    "validation_predict = predict_logistic(tx_validation, trained_weights)\n",
    "validation_labels = train_labels\n",
    "# s = 1, b = -1\n",
    "validation_labels = np.where(validation_labels > 0.5, 1, -1)\n",
    "print(validation_labels)\n",
    "print(validation_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6aa9b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(a, b):\n",
    "    return np.sum(a == b)/a.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5caaf7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686224\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(predict_validation, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "51110985",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, labels, ids = load_data_test('test.csv')\n",
    "test_data = standardize(clean_data(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c6922cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 24)\n",
      "(24,)\n",
      "[-1. -1. -1. ... -1. -1. -1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/xkb6gsv52xq9t6plg99f32v80000gn/T/ipykernel_1224/3839738373.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((test_data.shape[0], 1)), test_data]\n",
    "print(tx_test.shape)\n",
    "print(trained_weights.shape)\n",
    "predicted_labels = predict_logistic(tx_test, trained_weights)\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d28837f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_submission(ids, y_pred, name):\n",
    "\n",
    "    with open (name, 'w') as csvfile:\n",
    "        fd = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fd)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip (ids, y_pred):\n",
    "            writer.writerow({'Id' : int(r1), 'Prediction': str(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "50c95d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_submission(ids, predicted_labels, 'Predictions_Logistics_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef327135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
