{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import helpers as helpers\n",
    "import implementations as impl\n",
    "import cross_validation as cv\n",
    "import hyperparameter_opti as hpopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, train_labels, ids = helpers.load_data('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and standardize data\n",
    "new_data = helpers.standardize(helpers.clean_data(train_data))\n",
    "train_labels[train_labels==-1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add bias to data:\n",
    "new_data = np.c_[np.ones((new_data.shape[0], 1)), new_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Least Squares:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ls, loss_ls = impl.least_squares(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))\n",
    "tx_test = np.c_[np.ones((tx_test.shape[0], 1)), tx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = impl.test_data(weights_ls, tx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_LS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best degree to build a polynomial basis from the training data:\n",
    "degree_opt, lambda_opt = cv.best_degree_selection(train_labels, new_data, np.arange(2,5), 4, np.logspace(-4,0,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "print(degree_opt)\n",
    "print(lambda_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 31)\n",
      "(568238, 31)\n"
     ]
    }
   ],
   "source": [
    "print(new_data.shape)\n",
    "print(tx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000, 63)\n",
      "(63,)\n"
     ]
    }
   ],
   "source": [
    "#build polynomial basis from the entire training set using the optimal degree:\n",
    "xpoly = cv.build_poly(new_data, degree_opt)\n",
    "print(xpoly.shape)\n",
    "#compute weights and loss for the optimal lambda:\n",
    "w_ridge, loss_ridge = impl.ridge_regression(train_labels, xpoly, lambda_opt)\n",
    "print(w_ridge.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568238, 63)\n"
     ]
    }
   ],
   "source": [
    "xpoly_test = cv.build_poly(tx_test, degree_opt)\n",
    "print(xpoly_test.shape)\n",
    "y_test = impl.test_data(w_ridge, xpoly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, y_test, 'Predictions_RG.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice into training and validation sets\n",
    "y_validation, y_train, tx_validation, tx_train = helpers.slice_data(train_labels, new_data, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias to data\n",
    "tx_train = np.c_[np.ones((y_train.shape[0], 1)), tx_train]\n",
    "tx_validation = np.c_[np.ones((y_validation.shape[0], 1)), tx_validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [tx_train.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.7163567950814637\n",
      "Current iteration=100, loss=0.6227581517924388\n",
      "Current iteration=200, loss=0.584904560206431\n",
      "Current iteration=300, loss=0.5641307100362332\n",
      "Current iteration=400, loss=0.5512503018068265\n",
      "Current iteration=500, loss=0.5426562963788711\n",
      "Current iteration=600, loss=0.5366105979203447\n",
      "Current iteration=700, loss=0.5321775687371848\n",
      "Current iteration=800, loss=0.5288137652729488\n",
      "Current iteration=900, loss=0.526185117200647\n"
     ]
    }
   ],
   "source": [
    "#Find the value of gamma that minimizes the loss:\n",
    "gamma_opt = hpopt.best_gamma_selection(y_train, tx_train, 1000)\n",
    "# Train model\n",
    "trained_weights, train_loss = impl.logistic_regression(y_train, tx_train, initial_w, max_iters=1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "predict_validation = helpers.predict_logistic(tx_validation, trained_weights)\n",
    "predict_train = helpers.predict_logistic(tx_train, trained_weights)\n",
    "\n",
    "predict_validation[predict_validation == -1] = 0\n",
    "predict_train[predict_train == -1] = 0\n",
    "\n",
    "train_accuracy = helpers.accuracy(predict_train, y_train)\n",
    "validation_accuracy = helpers.accuracy(predict_validation, y_validation)\n",
    "\n",
    "print(f\"train_accuracy = {train_accuracy}\")\n",
    "print(f\"validation_accuracy = {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1.  1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), tx_test]\n",
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_Logistics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized Logistic Regression:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "train_labels, new_data = helpers.shuffle_data(train_labels, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [new_data.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250000,)\n",
      "(250000, 31)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)\n",
    "print(new_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights randomly according to a Gaussian distribution\n",
    "initial_w = np.random.normal(0., 0.1, [train_data.shape[1],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current gamma=1e-06\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.7782333628015712\n",
      "Current iteration=200, loss=0.7782143023450276\n",
      "Current iteration=300, loss=0.7781952445015405\n",
      "Current iteration=400, loss=0.7781761892708785\n",
      "Current iteration=500, loss=0.7781571366528102\n",
      "Current iteration=600, loss=0.7781380866471043\n",
      "Current iteration=700, loss=0.7781190392535294\n",
      "Current iteration=800, loss=0.7780999944718548\n",
      "Current iteration=900, loss=0.7780809523018483\n",
      "training_loss = 0.7780619127432786, validation_loss = 0.9494525485397384\n",
      "Current gamma=1e-05\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.7780619121559318\n",
      "Current iteration=200, loss=0.7778716595623911\n",
      "Current iteration=300, loss=0.7776816678591929\n",
      "Current iteration=400, loss=0.7774919368144765\n",
      "Current iteration=500, loss=0.7773024661961094\n",
      "Current iteration=600, loss=0.777113255771688\n",
      "Current iteration=700, loss=0.7769243053085393\n",
      "Current iteration=800, loss=0.7767356145737221\n",
      "Current iteration=900, loss=0.7765471833340293\n",
      "training_loss = 0.776359011355988, validation_loss = 0.9438469589338202\n",
      "Current gamma=0.0001\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.7763589532456963\n",
      "Current iteration=200, loss=0.7744913845845225\n",
      "Current iteration=300, loss=0.772649484738955\n",
      "Current iteration=400, loss=0.7708330160811412\n",
      "Current iteration=500, loss=0.7690417386501407\n",
      "Current iteration=600, loss=0.7672754102990664\n",
      "Current iteration=700, loss=0.7655337868432336\n",
      "Current iteration=800, loss=0.7638166222090101\n",
      "Current iteration=900, loss=0.7621236685830757\n",
      "training_loss = 0.7604546765617821, validation_loss = 0.8901244993064874\n",
      "Current gamma=0.001\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.7604494801447198\n",
      "Current iteration=200, loss=0.7450178471583785\n",
      "Current iteration=300, loss=0.7317029736555655\n",
      "Current iteration=400, loss=0.7202508932498823\n",
      "Current iteration=500, loss=0.7104192358170086\n",
      "Current iteration=600, loss=0.701984654049953\n",
      "Current iteration=700, loss=0.6947468739683501\n",
      "Current iteration=800, loss=0.6885300704271133\n",
      "Current iteration=900, loss=0.6831822916209859\n",
      "training_loss = 0.6785736326118601, validation_loss = 0.5345413950641368\n",
      "Current gamma=0.01\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.6784417147515561\n",
      "Current iteration=200, loss=0.6555084976051248\n",
      "Current iteration=300, loss=0.6490412145916296\n",
      "Current iteration=400, loss=0.6467035839778715\n",
      "Current iteration=500, loss=0.6456045222746882\n",
      "Current iteration=600, loss=0.6449634754319664\n",
      "Current iteration=700, loss=0.6445360789876983\n",
      "Current iteration=800, loss=0.6442296465954752\n",
      "Current iteration=900, loss=0.6440007389403785\n",
      "training_loss = 0.6438251166034, validation_loss = 0.19843804485504898\n",
      "Current gamma=0.1\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.6438180008112009\n",
      "Current iteration=200, loss=0.6431684995200584\n",
      "Current iteration=300, loss=0.6430190525397934\n",
      "Current iteration=400, loss=0.6429606342900527\n",
      "Current iteration=500, loss=0.6429335267004616\n",
      "Current iteration=600, loss=0.6429200816924958\n",
      "Current iteration=700, loss=0.6429131398547423\n",
      "Current iteration=800, loss=0.6429094419199539\n",
      "Current iteration=900, loss=0.642907418698683\n",
      "breaking threshold\n",
      "training_loss = 0.6429065406227484, validation_loss = 0.19809828382215391\n",
      "Current gamma=1.0\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=0.642906172133293\n",
      "breaking threshold\n",
      "training_loss = 0.6429048265486842, validation_loss = 0.1981056115456276\n",
      "Current gamma=10.0\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=2.096889837163794\n",
      "Current iteration=200, loss=2.862320419820072\n",
      "Current iteration=300, loss=5.941179250159228\n",
      "Current iteration=400, loss=2.0918952944434426\n",
      "Current iteration=500, loss=2.8725440294675946\n",
      "Current iteration=600, loss=5.938667315318852\n",
      "Current iteration=700, loss=2.09004907898887\n",
      "Current iteration=800, loss=2.8759683375293297\n",
      "Current iteration=900, loss=5.937635551202984\n",
      "training_loss = 2.0893982378244615, validation_loss = -2.033569993124613\n",
      "Current gamma=100.0\n",
      "Current iteration=0, loss=0.778252425871402\n",
      "Current iteration=100, loss=7.5537847299483145\n",
      "Current iteration=200, loss=55.2531211660088\n",
      "Current iteration=300, loss=10.70393210891607\n",
      "Current iteration=400, loss=36.433602074563744\n",
      "Current iteration=500, loss=19.89353856508895\n",
      "Current iteration=600, loss=30.600500161921715\n",
      "Current iteration=700, loss=55.13929351575965\n",
      "Current iteration=800, loss=16.787686740952005\n",
      "Current iteration=900, loss=49.654421904018136\n",
      "training_loss = 41.05262002448483, validation_loss = 75.92270542136369\n",
      "Best gamma = 10.0, training_loss = 2.0893982378244615, validation_loss = -2.033569993124613\n",
      "Current lambda=1e-06\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 2.0893972705220656, validation_loss = -2.0335522672873423\n",
      "Current lambda=1e-05\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 2.089361140134956, validation_loss = -2.033364833938735\n",
      "Current lambda=0.0001\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 2.0876189355535173, validation_loss = -2.030090798703584\n",
      "Current lambda=0.001\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 2.042174424659772, validation_loss = -1.969377145812204\n",
      "Current lambda=0.01\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 2.898412034513743, validation_loss = 0.5839872079453201\n",
      "Current lambda=0.1\n",
      "Current it = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:65: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1 + np.exp(-t))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = 10601.812560597451, validation_loss = 15784.931112396547\n",
      "Current lambda=1.0\n",
      "Current it = 0\n",
      "Current it = 100\n",
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:93: RuntimeWarning: invalid value encountered in logaddexp\n",
      "  loss = np.sum(np.logaddexp(0, tx @ w) - y * tx.dot(w))/tx.shape[0]\n",
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\hyperparameter_opti.py:30: RuntimeWarning: invalid value encountered in logaddexp\n",
      "  loss_validation = (np.sum(np.logaddexp(0, tx_va.dot(w)) + y_va * tx_va.dot(w)) + lambda_*np.linalg.norm(w)**2)/y_va.shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss = nan, validation_loss = nan\n",
      "Current lambda=10.0\n",
      "Current it = 0\n",
      "Current it = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manon\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py:75: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = (tx.T.dot(sigmoid(tx.dot(w)) - y))/tx.shape[0] + 2*lambda_*w\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current it = 200\n",
      "Current it = 300\n",
      "Current it = 400\n",
      "Current it = 500\n",
      "Current it = 600\n",
      "Current it = 700\n",
      "Current it = 800\n",
      "Current it = 900\n",
      "training_loss = nan, validation_loss = nan\n",
      "Best lambda = 1.0, training_loss = nan, validation_loss = nan\n"
     ]
    }
   ],
   "source": [
    "#Find the most optimal values for the regularization term (lambda) and gamma:\n",
    "gamma_opt = hpopt.best_gamma_selection(train_labels, train_data, 1000)\n",
    "lambda_opt = hpopt.best_lambda_selection(train_labels, train_data, 1000, gamma=gamma_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (250000,31) and (32,) not aligned: 31 (dim 1) != 32 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-02b2dd77c2bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrained_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\Master SV\\MA1\\ML\\ml-project-1-md_am_af-project1\\implementations.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m#loss = np.sum(np.logaddexp(0, tx @ w) - y * tx.dot(w))/tx.shape[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;31m# Compute gradient with regularization term\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;31m# Apply GD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (250000,31) and (32,) not aligned: 31 (dim 1) != 32 (dim 0)"
     ]
    }
   ],
   "source": [
    "trained_weights, train_loss = impl.reg_logistic_regression(train_labels, new_data, 0.01, initial_w, max_iters=1000, gamma=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation:\n",
    "predict_validation = helpers.predict_logistic(tx_validation, trained_weights)\n",
    "predict_train = helpers.predict_logistic(tx_train, trained_weights)\n",
    "\n",
    "predict_validation[predict_validation == -1] = 0\n",
    "predict_train[predict_train == -1] = 0\n",
    "\n",
    "train_accuracy = helpers.accuracy(predict_train, y_train)\n",
    "validation_accuracy = helpers.accuracy(predict_validation, y_validation)\n",
    "\n",
    "print(f\"train_accuracy = {train_accuracy}\")\n",
    "print(f\"validation_accuracy = {validation_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test, y_test, ids_test = helpers.load_data('test.csv', train=False)\n",
    "tx_test = helpers.standardize(helpers.clean_data(tx_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_test = np.c_[np.ones((y_test.shape[0], 1)), tx_test]\n",
    "predict_test = helpers.predict_logistic(tx_test, trained_weights)\n",
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.create_csv_submission(ids_test, predict_test, 'Predictions_RegLogistics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
